Name                                                                                     Type                   Forward_MACs     FPercent    Memory          MPercent    Params      PPercent    InShape        OutShape
---------------------------------------------------------------------------------------  ---------------------  ---------------  ----------  --------------  ----------  ----------  ----------  -------------  -------------
/backbone/stem/conv1/Conv                                                                Conv                   2,483,027,968    0.92%       67,146,752      0.30%       9,472       0.01%       1x3x1024x1024  1x64x512x512
/backbone/stem/Relu                                                                      Relu                   16,777,216       0.01%       67,108,864      0.30%       0           0.00%       1x64x512x512   1x64x512x512
/backbone/stem/MaxPool                                                                   MaxPool                37,748,736       0.01%       16,777,216      0.08%       0           0.00%       1x64x512x512   1x64x256x256
/backbone/res2/res2.0/conv1/Conv                                                         Conv                   272,629,760      0.10%       16,793,856      0.08%       4,160       0.01%       1x64x256x256   1x64x256x256
/backbone/res2/res2.0/shortcut/Conv                                                      Conv                   1,090,519,040    0.41%       67,175,424      0.30%       16,640      0.02%       1x64x256x256   1x256x256x256
/backbone/res2/res2.0/Relu                                                               Relu                   4,194,304        0.00%       16,777,216      0.08%       0           0.00%       1x64x256x256   1x64x256x256
/backbone/res2/res2.0/conv2/Conv                                                         Conv                   2,420,113,408    0.90%       16,924,928      0.08%       36,928      0.05%       1x64x256x256   1x64x256x256
/backbone/res2/res2.0/Relu_1                                                             Relu                   4,194,304        0.00%       16,777,216      0.08%       0           0.00%       1x64x256x256   1x64x256x256
/backbone/res2/res2.0/conv3/Conv                                                         Conv                   1,090,519,040    0.41%       67,175,424      0.30%       16,640      0.02%       1x64x256x256   1x256x256x256
/backbone/res2/res2.0/Add                                                                Add                    16,777,216       0.01%       67,108,864      0.30%       0           0.00%       1x256x256x256  1x256x256x256
/backbone/res2/res2.0/Relu_2                                                             Relu                   16,777,216       0.01%       67,108,864      0.30%       0           0.00%       1x256x256x256  1x256x256x256
/backbone/res2/res2.1/conv1/Conv                                                         Conv                   1,077,936,128    0.40%       16,843,008      0.08%       16,448      0.02%       1x256x256x256  1x64x256x256
/backbone/res2/res2.1/Relu                                                               Relu                   4,194,304        0.00%       16,777,216      0.08%       0           0.00%       1x64x256x256   1x64x256x256
/backbone/res2/res2.1/conv2/Conv                                                         Conv                   2,420,113,408    0.90%       16,924,928      0.08%       36,928      0.05%       1x64x256x256   1x64x256x256
/backbone/res2/res2.1/Relu_1                                                             Relu                   4,194,304        0.00%       16,777,216      0.08%       0           0.00%       1x64x256x256   1x64x256x256
/backbone/res2/res2.1/conv3/Conv                                                         Conv                   1,090,519,040    0.41%       67,175,424      0.30%       16,640      0.02%       1x64x256x256   1x256x256x256
/backbone/res2/res2.1/Add                                                                Add                    16,777,216       0.01%       67,108,864      0.30%       0           0.00%       1x256x256x256  1x256x256x256
/backbone/res2/res2.1/Relu_2                                                             Relu                   16,777,216       0.01%       67,108,864      0.30%       0           0.00%       1x256x256x256  1x256x256x256
/backbone/res2/res2.2/conv1/Conv                                                         Conv                   1,077,936,128    0.40%       16,843,008      0.08%       16,448      0.02%       1x256x256x256  1x64x256x256
/backbone/res2/res2.2/Relu                                                               Relu                   4,194,304        0.00%       16,777,216      0.08%       0           0.00%       1x64x256x256   1x64x256x256
/backbone/res2/res2.2/conv2/Conv                                                         Conv                   2,420,113,408    0.90%       16,924,928      0.08%       36,928      0.05%       1x64x256x256   1x64x256x256
/backbone/res2/res2.2/Relu_1                                                             Relu                   4,194,304        0.00%       16,777,216      0.08%       0           0.00%       1x64x256x256   1x64x256x256
/backbone/res2/res2.2/conv3/Conv                                                         Conv                   1,090,519,040    0.41%       67,175,424      0.30%       16,640      0.02%       1x64x256x256   1x256x256x256
/backbone/res2/res2.2/Add                                                                Add                    16,777,216       0.01%       67,108,864      0.30%       0           0.00%       1x256x256x256  1x256x256x256
/backbone/res2/res2.2/Relu_2                                                             Relu                   16,777,216       0.01%       67,108,864      0.30%       0           0.00%       1x256x256x256  1x256x256x256
/backbone/res3/res3.0/conv1/Conv                                                         Conv                   2,155,872,256    0.80%       33,686,016      0.15%       32,896      0.04%       1x256x256x256  1x128x256x256
/backbone/res3/res3.0/shortcut/Conv                                                      Conv                   2,155,872,256    0.80%       34,080,768      0.15%       131,584     0.16%       1x256x256x256  1x512x128x128
/sem_seg_head/adapter_1/Conv                                                             Conv                   4,294,967,296    1.60%       67,371,008      0.30%       65,536      0.08%       1x256x256x256  1x256x256x256
/backbone/res3/res3.0/Relu                                                               Relu                   8,388,608        0.00%       33,554,432      0.15%       0           0.00%       1x128x256x256  1x128x256x256
/backbone/res3/res3.0/conv2/Conv                                                         Conv                   2,418,016,256    0.90%       8,978,944       0.04%       147,584     0.18%       1x128x256x256  1x128x128x128
/sem_seg_head/adapter_1/norm/InstanceNormalization                                       InstanceNormalization  117,440,512      0.04%       67,109,120      0.30%       64          0.00%       1x32x524288    1x32x524288
/backbone/res3/res3.0/Relu_1                                                             Relu                   2,097,152        0.00%       8,388,608       0.04%       0           0.00%       1x128x128x128  1x128x128x128
/backbone/res3/res3.0/conv3/Conv                                                         Conv                   1,082,130,432    0.40%       33,818,624      0.15%       66,048      0.08%       1x128x128x128  1x512x128x128
/sem_seg_head/adapter_1/norm/Mul                                                         Mul                    16,777,216       0.01%       67,109,888      0.30%       256         0.00%       1x256x256x256  1x256x256x256
/backbone/res3/res3.0/Add                                                                Add                    8,388,608        0.00%       33,554,432      0.15%       0           0.00%       1x512x128x128  1x512x128x128
/sem_seg_head/adapter_1/norm/Add                                                         Add                    16,777,216       0.01%       67,109,888      0.30%       256         0.00%       1x256x256x256  1x256x256x256
/backbone/res3/res3.0/Relu_2                                                             Relu                   8,388,608        0.00%       33,554,432      0.15%       0           0.00%       1x512x128x128  1x512x128x128
/backbone/res3/res3.1/conv1/Conv                                                         Conv                   1,075,838,976    0.40%       8,651,264       0.04%       65,664      0.08%       1x512x128x128  1x128x128x128
/backbone/res3/res3.1/Relu                                                               Relu                   2,097,152        0.00%       8,388,608       0.04%       0           0.00%       1x128x128x128  1x128x128x128
/backbone/res3/res3.1/conv2/Conv                                                         Conv                   2,418,016,256    0.90%       8,978,944       0.04%       147,584     0.18%       1x128x128x128  1x128x128x128
/backbone/res3/res3.1/Relu_1                                                             Relu                   2,097,152        0.00%       8,388,608       0.04%       0           0.00%       1x128x128x128  1x128x128x128
/backbone/res3/res3.1/conv3/Conv                                                         Conv                   1,082,130,432    0.40%       33,818,624      0.15%       66,048      0.08%       1x128x128x128  1x512x128x128
/backbone/res3/res3.1/Add                                                                Add                    8,388,608        0.00%       33,554,432      0.15%       0           0.00%       1x512x128x128  1x512x128x128
/backbone/res3/res3.1/Relu_2                                                             Relu                   8,388,608        0.00%       33,554,432      0.15%       0           0.00%       1x512x128x128  1x512x128x128
/backbone/res3/res3.2/conv1/Conv                                                         Conv                   1,075,838,976    0.40%       8,651,264       0.04%       65,664      0.08%       1x512x128x128  1x128x128x128
/backbone/res3/res3.2/Relu                                                               Relu                   2,097,152        0.00%       8,388,608       0.04%       0           0.00%       1x128x128x128  1x128x128x128
/backbone/res3/res3.2/conv2/Conv                                                         Conv                   2,418,016,256    0.90%       8,978,944       0.04%       147,584     0.18%       1x128x128x128  1x128x128x128
/backbone/res3/res3.2/Relu_1                                                             Relu                   2,097,152        0.00%       8,388,608       0.04%       0           0.00%       1x128x128x128  1x128x128x128
/backbone/res3/res3.2/conv3/Conv                                                         Conv                   1,082,130,432    0.40%       33,818,624      0.15%       66,048      0.08%       1x128x128x128  1x512x128x128
/backbone/res3/res3.2/Add                                                                Add                    8,388,608        0.00%       33,554,432      0.15%       0           0.00%       1x512x128x128  1x512x128x128
/backbone/res3/res3.2/Relu_2                                                             Relu                   8,388,608        0.00%       33,554,432      0.15%       0           0.00%       1x512x128x128  1x512x128x128
/backbone/res3/res3.3/conv1/Conv                                                         Conv                   1,075,838,976    0.40%       8,651,264       0.04%       65,664      0.08%       1x512x128x128  1x128x128x128
/backbone/res3/res3.3/Relu                                                               Relu                   2,097,152        0.00%       8,388,608       0.04%       0           0.00%       1x128x128x128  1x128x128x128
/backbone/res3/res3.3/conv2/Conv                                                         Conv                   2,418,016,256    0.90%       8,978,944       0.04%       147,584     0.18%       1x128x128x128  1x128x128x128
/backbone/res3/res3.3/Relu_1                                                             Relu                   2,097,152        0.00%       8,388,608       0.04%       0           0.00%       1x128x128x128  1x128x128x128
/backbone/res3/res3.3/conv3/Conv                                                         Conv                   1,082,130,432    0.40%       33,818,624      0.15%       66,048      0.08%       1x128x128x128  1x512x128x128
/backbone/res3/res3.3/Add                                                                Add                    8,388,608        0.00%       33,554,432      0.15%       0           0.00%       1x512x128x128  1x512x128x128
/backbone/res3/res3.3/Relu_2                                                             Relu                   8,388,608        0.00%       33,554,432      0.15%       0           0.00%       1x512x128x128  1x512x128x128
/backbone/res4/res4.0/conv1/Conv                                                         Conv                   2,151,677,952    0.80%       17,302,528      0.08%       131,328     0.16%       1x512x128x128  1x256x128x128
/backbone/res4/res4.0/shortcut/Conv                                                      Conv                   2,151,677,952    0.80%       18,878,464      0.09%       525,312     0.65%       1x512x128x128  1x1024x64x64
/sem_seg_head/input_proj.2/input_proj.2.0/Conv                                           Conv                   2,151,677,952    0.80%       17,302,528      0.08%       131,328     0.16%       1x512x128x128  1x256x128x128
/backbone/res4/res4.0/Relu                                                               Relu                   4,194,304        0.00%       16,777,216      0.08%       0           0.00%       1x256x128x128  1x256x128x128
/backbone/res4/res4.0/conv2/Conv                                                         Conv                   2,416,967,680    0.90%       6,554,624       0.03%       590,080     0.73%       1x256x128x128  1x256x64x64
/sem_seg_head/input_proj.2/input_proj.2.1/InstanceNormalization                          InstanceNormalization  29,360,128       0.01%       16,777,216      0.08%       0           0.00%       1x32x131072    1x32x131072
/backbone/res4/res4.0/Relu_1                                                             Relu                   1,048,576        0.00%       4,194,304       0.02%       0           0.00%       1x256x64x64    1x256x64x64
/backbone/res4/res4.0/conv3/Conv                                                         Conv                   1,077,936,128    0.40%       17,829,888      0.08%       263,168     0.33%       1x256x64x64    1x1024x64x64
/sem_seg_head/input_proj.2/input_proj.2.1/Mul                                            Mul                    4,194,304        0.00%       16,778,240      0.08%       256         0.00%       1x256x128x128  1x256x128x128
/backbone/res4/res4.0/Add                                                                Add                    4,194,304        0.00%       16,777,216      0.08%       0           0.00%       1x1024x64x64   1x1024x64x64
/sem_seg_head/input_proj.2/input_proj.2.1/Add                                            Add                    4,194,304        0.00%       16,778,240      0.08%       256         0.00%       1x256x128x128  1x256x128x128
/backbone/res4/res4.0/Relu_2                                                             Relu                   4,194,304        0.00%       16,777,216      0.08%       0           0.00%       1x1024x64x64   1x1024x64x64
/backbone/res4/res4.1/conv1/Conv                                                         Conv                   1,074,790,400    0.40%       5,243,904       0.02%       262,400     0.33%       1x1024x64x64   1x256x64x64
/sem_seg_head/transformer/Transpose_4                                                    Transpose              0                0.00%       16,777,216      0.08%       0           0.00%       1x256x16384    1x16384x256
/backbone/res4/res4.1/Relu                                                               Relu                   1,048,576        0.00%       4,194,304       0.02%       0           0.00%       1x256x64x64    1x256x64x64
/backbone/res4/res4.1/conv2/Conv                                                         Conv                   2,416,967,680    0.90%       6,554,624       0.03%       590,080     0.73%       1x256x64x64    1x256x64x64
/backbone/res4/res4.1/Relu_1                                                             Relu                   1,048,576        0.00%       4,194,304       0.02%       0           0.00%       1x256x64x64    1x256x64x64
/backbone/res4/res4.1/conv3/Conv                                                         Conv                   1,077,936,128    0.40%       17,829,888      0.08%       263,168     0.33%       1x256x64x64    1x1024x64x64
/backbone/res4/res4.1/Add                                                                Add                    4,194,304        0.00%       16,777,216      0.08%       0           0.00%       1x1024x64x64   1x1024x64x64
/backbone/res4/res4.1/Relu_2                                                             Relu                   4,194,304        0.00%       16,777,216      0.08%       0           0.00%       1x1024x64x64   1x1024x64x64
/backbone/res4/res4.2/conv1/Conv                                                         Conv                   1,074,790,400    0.40%       5,243,904       0.02%       262,400     0.33%       1x1024x64x64   1x256x64x64
/backbone/res4/res4.2/Relu                                                               Relu                   1,048,576        0.00%       4,194,304       0.02%       0           0.00%       1x256x64x64    1x256x64x64
/backbone/res4/res4.2/conv2/Conv                                                         Conv                   2,416,967,680    0.90%       6,554,624       0.03%       590,080     0.73%       1x256x64x64    1x256x64x64
/backbone/res4/res4.2/Relu_1                                                             Relu                   1,048,576        0.00%       4,194,304       0.02%       0           0.00%       1x256x64x64    1x256x64x64
/backbone/res4/res4.2/conv3/Conv                                                         Conv                   1,077,936,128    0.40%       17,829,888      0.08%       263,168     0.33%       1x256x64x64    1x1024x64x64
/backbone/res4/res4.2/Add                                                                Add                    4,194,304        0.00%       16,777,216      0.08%       0           0.00%       1x1024x64x64   1x1024x64x64
/backbone/res4/res4.2/Relu_2                                                             Relu                   4,194,304        0.00%       16,777,216      0.08%       0           0.00%       1x1024x64x64   1x1024x64x64
/backbone/res4/res4.3/conv1/Conv                                                         Conv                   1,074,790,400    0.40%       5,243,904       0.02%       262,400     0.33%       1x1024x64x64   1x256x64x64
/backbone/res4/res4.3/Relu                                                               Relu                   1,048,576        0.00%       4,194,304       0.02%       0           0.00%       1x256x64x64    1x256x64x64
/backbone/res4/res4.3/conv2/Conv                                                         Conv                   2,416,967,680    0.90%       6,554,624       0.03%       590,080     0.73%       1x256x64x64    1x256x64x64
/backbone/res4/res4.3/Relu_1                                                             Relu                   1,048,576        0.00%       4,194,304       0.02%       0           0.00%       1x256x64x64    1x256x64x64
/backbone/res4/res4.3/conv3/Conv                                                         Conv                   1,077,936,128    0.40%       17,829,888      0.08%       263,168     0.33%       1x256x64x64    1x1024x64x64
/backbone/res4/res4.3/Add                                                                Add                    4,194,304        0.00%       16,777,216      0.08%       0           0.00%       1x1024x64x64   1x1024x64x64
/backbone/res4/res4.3/Relu_2                                                             Relu                   4,194,304        0.00%       16,777,216      0.08%       0           0.00%       1x1024x64x64   1x1024x64x64
/backbone/res4/res4.4/conv1/Conv                                                         Conv                   1,074,790,400    0.40%       5,243,904       0.02%       262,400     0.33%       1x1024x64x64   1x256x64x64
/backbone/res4/res4.4/Relu                                                               Relu                   1,048,576        0.00%       4,194,304       0.02%       0           0.00%       1x256x64x64    1x256x64x64
/backbone/res4/res4.4/conv2/Conv                                                         Conv                   2,416,967,680    0.90%       6,554,624       0.03%       590,080     0.73%       1x256x64x64    1x256x64x64
/backbone/res4/res4.4/Relu_1                                                             Relu                   1,048,576        0.00%       4,194,304       0.02%       0           0.00%       1x256x64x64    1x256x64x64
/backbone/res4/res4.4/conv3/Conv                                                         Conv                   1,077,936,128    0.40%       17,829,888      0.08%       263,168     0.33%       1x256x64x64    1x1024x64x64
/backbone/res4/res4.4/Add                                                                Add                    4,194,304        0.00%       16,777,216      0.08%       0           0.00%       1x1024x64x64   1x1024x64x64
/backbone/res4/res4.4/Relu_2                                                             Relu                   4,194,304        0.00%       16,777,216      0.08%       0           0.00%       1x1024x64x64   1x1024x64x64
/backbone/res4/res4.5/conv1/Conv                                                         Conv                   1,074,790,400    0.40%       5,243,904       0.02%       262,400     0.33%       1x1024x64x64   1x256x64x64
/backbone/res4/res4.5/Relu                                                               Relu                   1,048,576        0.00%       4,194,304       0.02%       0           0.00%       1x256x64x64    1x256x64x64
/backbone/res4/res4.5/conv2/Conv                                                         Conv                   2,416,967,680    0.90%       6,554,624       0.03%       590,080     0.73%       1x256x64x64    1x256x64x64
/backbone/res4/res4.5/Relu_1                                                             Relu                   1,048,576        0.00%       4,194,304       0.02%       0           0.00%       1x256x64x64    1x256x64x64
/backbone/res4/res4.5/conv3/Conv                                                         Conv                   1,077,936,128    0.40%       17,829,888      0.08%       263,168     0.33%       1x256x64x64    1x1024x64x64
/backbone/res4/res4.5/Add                                                                Add                    4,194,304        0.00%       16,777,216      0.08%       0           0.00%       1x1024x64x64   1x1024x64x64
/backbone/res4/res4.5/Relu_2                                                             Relu                   4,194,304        0.00%       16,777,216      0.08%       0           0.00%       1x1024x64x64   1x1024x64x64
/backbone/res5/res5.0/conv1/Conv                                                         Conv                   2,149,580,800    0.80%       10,487,808      0.05%       524,800     0.65%       1x1024x64x64   1x512x64x64
/backbone/res5/res5.0/shortcut/Conv                                                      Conv                   2,149,580,800    0.80%       16,785,408      0.08%       2,099,200   2.61%       1x1024x64x64   1x2048x32x32
/sem_seg_head/input_proj.1/input_proj.1.0/Conv                                           Conv                   1,074,790,400    0.40%       5,243,904       0.02%       262,400     0.33%       1x1024x64x64   1x256x64x64
/backbone/res5/res5.0/Relu                                                               Relu                   2,097,152        0.00%       8,388,608       0.04%       0           0.00%       1x512x64x64    1x512x64x64
/backbone/res5/res5.0/conv2/Conv                                                         Conv                   2,416,443,392    0.90%       11,536,384      0.05%       2,359,808   2.93%       1x512x64x64    1x512x32x32
/sem_seg_head/input_proj.1/input_proj.1.1/InstanceNormalization                          InstanceNormalization  7,340,032        0.00%       4,194,304       0.02%       0           0.00%       1x32x32768     1x32x32768
/backbone/res5/res5.0/Relu_1                                                             Relu                   524,288          0.00%       2,097,152       0.01%       0           0.00%       1x512x32x32    1x512x32x32
/backbone/res5/res5.0/conv3/Conv                                                         Conv                   1,075,838,976    0.40%       12,591,104      0.06%       1,050,624   1.31%       1x512x32x32    1x2048x32x32
/sem_seg_head/input_proj.1/input_proj.1.1/Mul                                            Mul                    1,048,576        0.00%       4,195,328       0.02%       256         0.00%       1x256x64x64    1x256x64x64
/backbone/res5/res5.0/Add                                                                Add                    2,097,152        0.00%       8,388,608       0.04%       0           0.00%       1x2048x32x32   1x2048x32x32
/sem_seg_head/input_proj.1/input_proj.1.1/Add                                            Add                    1,048,576        0.00%       4,195,328       0.02%       256         0.00%       1x256x64x64    1x256x64x64
/backbone/res5/res5.0/Relu_2                                                             Relu                   2,097,152        0.00%       8,388,608       0.04%       0           0.00%       1x2048x32x32   1x2048x32x32
/backbone/res5/res5.1/conv1/Conv                                                         Conv                   1,074,266,112    0.40%       6,293,504       0.03%       1,049,088   1.30%       1x2048x32x32   1x512x32x32
/sem_seg_head/transformer/Transpose_2                                                    Transpose              0                0.00%       4,194,304       0.02%       0           0.00%       1x256x4096     1x4096x256
/backbone/res5/res5.1/Relu                                                               Relu                   524,288          0.00%       2,097,152       0.01%       0           0.00%       1x512x32x32    1x512x32x32
/backbone/res5/res5.1/conv2/Conv                                                         Conv                   2,416,443,392    0.90%       11,536,384      0.05%       2,359,808   2.93%       1x512x32x32    1x512x32x32
/backbone/res5/res5.1/Relu_1                                                             Relu                   524,288          0.00%       2,097,152       0.01%       0           0.00%       1x512x32x32    1x512x32x32
/backbone/res5/res5.1/conv3/Conv                                                         Conv                   1,075,838,976    0.40%       12,591,104      0.06%       1,050,624   1.31%       1x512x32x32    1x2048x32x32
/backbone/res5/res5.1/Add                                                                Add                    2,097,152        0.00%       8,388,608       0.04%       0           0.00%       1x2048x32x32   1x2048x32x32
/backbone/res5/res5.1/Relu_2                                                             Relu                   2,097,152        0.00%       8,388,608       0.04%       0           0.00%       1x2048x32x32   1x2048x32x32
/backbone/res5/res5.2/conv1/Conv                                                         Conv                   1,074,266,112    0.40%       6,293,504       0.03%       1,049,088   1.30%       1x2048x32x32   1x512x32x32
/backbone/res5/res5.2/Relu                                                               Relu                   524,288          0.00%       2,097,152       0.01%       0           0.00%       1x512x32x32    1x512x32x32
/backbone/res5/res5.2/conv2/Conv                                                         Conv                   2,416,443,392    0.90%       11,536,384      0.05%       2,359,808   2.93%       1x512x32x32    1x512x32x32
/backbone/res5/res5.2/Relu_1                                                             Relu                   524,288          0.00%       2,097,152       0.01%       0           0.00%       1x512x32x32    1x512x32x32
/backbone/res5/res5.2/conv3/Conv                                                         Conv                   1,075,838,976    0.40%       12,591,104      0.06%       1,050,624   1.31%       1x512x32x32    1x2048x32x32
/backbone/res5/res5.2/Add                                                                Add                    2,097,152        0.00%       8,388,608       0.04%       0           0.00%       1x2048x32x32   1x2048x32x32
/backbone/res5/res5.2/Relu_2                                                             Relu                   2,097,152        0.00%       8,388,608       0.04%       0           0.00%       1x2048x32x32   1x2048x32x32
/sem_seg_head/input_proj.0/input_proj.0.0/Conv                                           Conv                   537,133,056      0.20%       3,146,752       0.01%       524,544     0.65%       1x2048x32x32   1x256x32x32
/sem_seg_head/input_proj.0/input_proj.0.1/InstanceNormalization                          InstanceNormalization  1,835,008        0.00%       1,048,576       0.00%       0           0.00%       1x32x8192      1x32x8192
/sem_seg_head/input_proj.0/input_proj.0.1/Mul                                            Mul                    262,144          0.00%       1,049,600       0.00%       256         0.00%       1x256x32x32    1x256x32x32
/sem_seg_head/input_proj.0/input_proj.0.1/Add                                            Add                    262,144          0.00%       1,049,600       0.00%       256         0.00%       1x256x32x32    1x256x32x32
/sem_seg_head/transformer/Transpose                                                      Transpose              0                0.00%       1,048,576       0.00%       0           0.00%       1x256x1024     1x1024x256
/sem_seg_head/transformer/encoder/layers.0/Add                                           Add                    5,505,024        0.00%       44,040,192      0.20%       5,505,024   6.84%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.0/self_attn/value_proj/MatMul                   MatMul                 1,409,286,144    0.52%       22,282,240      0.10%       65,536      0.08%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.0/self_attn/value_proj/Add                      Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       256            1x21504x256
/sem_seg_head/transformer/encoder/layers.0/self_attn/sampling_offsets/MatMul             MatMul                 1,056,964,608    0.39%       16,711,680      0.08%       49,152      0.06%       1x21504x256    1x21504x192
/sem_seg_head/transformer/encoder/layers.0/self_attn/attention_weights/MatMul            MatMul                 528,482,304      0.20%       8,355,840       0.04%       24,576      0.03%       1x21504x256    1x21504x96
/sem_seg_head/transformer/encoder/layers.0/self_attn/Where                               Where                  0                0.00%       22,041,600      0.10%       21,504      0.03%       1x21504x1      1x21504x256
/sem_seg_head/transformer/encoder/layers.0/self_attn/sampling_offsets/Add                Add                    4,128,768        0.00%       16,515,840      0.07%       192         0.00%       192            1x21504x192
/sem_seg_head/transformer/encoder/layers.0/self_attn/attention_weights/Add               Add                    2,064,384        0.00%       8,257,920       0.04%       96          0.00%       96             1x21504x96
/sem_seg_head/transformer/encoder/layers.0/self_attn/Div                                 Div                    16,515,072       0.01%       33,030,144      0.15%       4,128,768   5.13%       1x21504x192    1x21504x192
/sem_seg_head/transformer/encoder/layers.0/self_attn/Softmax                             Softmax                74,317,824       0.03%       8,257,536       0.04%       0           0.00%       1x21504x8x12   1x21504x8x12
/sem_seg_head/transformer/encoder/layers.0/self_attn/Add                                 Add                    4,128,768        0.00%       33,030,144      0.15%       4,128,768   5.13%       1x21504x192    1x21504x192
/sem_seg_head/transformer/encoder/layers.0/self_attn/Split                               Split                  0                0.00%       22,020,120      0.10%       3           0.00%       1x21504x8x32   1x1024x8x32
/sem_seg_head/transformer/encoder/layers.0/self_attn/Mul_2                               Mul                    4,128,768        0.00%       16,515,072      0.07%       0           0.00%       1x21504x192    1x21504x192
/sem_seg_head/transformer/encoder/layers.0/self_attn/Sub                                 Sub                    4,128,768        0.00%       16,515,072      0.07%       0           0.00%       1x21504x192    1x21504x192
/sem_seg_head/transformer/encoder/layers.0/self_attn/Transpose                           Transpose              0                0.00%       1,048,576       0.00%       0           0.00%       1x1024x256     1x256x1024
/sem_seg_head/transformer/encoder/layers.0/self_attn/Transpose_2                         Transpose              0                0.00%       4,194,304       0.02%       0           0.00%       1x4096x256     1x256x4096
/sem_seg_head/transformer/encoder/layers.0/self_attn/Transpose_4                         Transpose              0                0.00%       16,777,216      0.08%       0           0.00%       1x16384x256    1x256x16384
/sem_seg_head/transformer/encoder/layers.0/self_attn/Transpose_6                         Transpose              0                0.00%       8,257,536       0.04%       0           0.00%       1x21504x8x3x4  1x8x21504x3x4
/sem_seg_head/transformer/encoder/layers.0/self_attn/Transpose_1                         Transpose              0                0.00%       5,505,024       0.02%       0           0.00%       1x21504x8x4x2  1x8x21504x4x2
/sem_seg_head/transformer/encoder/layers.0/self_attn/Transpose_3                         Transpose              0                0.00%       5,505,024       0.02%       0           0.00%       1x21504x8x4x2  1x8x21504x4x2
/sem_seg_head/transformer/encoder/layers.0/self_attn/Transpose_5                         Transpose              0                0.00%       5,505,024       0.02%       0           0.00%       1x21504x8x4x2  1x8x21504x4x2
/sem_seg_head/transformer/encoder/layers.0/self_attn/GridSample                          GridSample             0                0.00%       88,080,384      0.40%       0           0.00%       8x32x32x32     8x32x21504x4
/sem_seg_head/transformer/encoder/layers.0/self_attn/GridSample_1                        GridSample             0                0.00%       88,080,384      0.40%       0           0.00%       8x32x64x64     8x32x21504x4
/sem_seg_head/transformer/encoder/layers.0/self_attn/GridSample_2                        GridSample             0                0.00%       88,080,384      0.40%       0           0.00%       8x32x128x128   8x32x21504x4
/sem_seg_head/transformer/encoder/layers.0/self_attn/Mul_3                               Mul                    66,060,288       0.02%       264,241,152     1.19%       0           0.00%       8x32x21504x12  8x32x21504x12
/sem_seg_head/transformer/encoder/layers.0/self_attn/ReduceSum                           ReduceSum              66,060,288       0.02%       22,020,104      0.10%       1           0.00%       8x32x21504x12  8x32x21504
/sem_seg_head/transformer/encoder/layers.0/self_attn/Transpose_7                         Transpose              0                0.00%       22,020,096      0.10%       0           0.00%       1x256x21504    1x21504x256
/sem_seg_head/transformer/encoder/layers.0/self_attn/output_proj/MatMul                  MatMul                 1,409,286,144    0.52%       22,282,240      0.10%       65,536      0.08%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.0/self_attn/output_proj/Add                     Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       256            1x21504x256
/sem_seg_head/transformer/encoder/layers.0/Add_1                                         Add                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.0/norm1/ReduceMean                              ReduceMean             5,505,024        0.00%       86,016          0.00%       0           0.00%       1x21504x256    1x21504x1
/sem_seg_head/transformer/encoder/layers.0/norm1/Sub                                     Sub                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.0/norm1/Pow                                     Pow                    176,160,768      0.07%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.0/norm1/ReduceMean_1                            ReduceMean             5,505,024        0.00%       86,016          0.00%       0           0.00%       1x21504x256    1x21504x1
/sem_seg_head/transformer/encoder/layers.0/norm1/Add                                     Add                    21,504           0.00%       86,016          0.00%       0           0.00%       1x21504x1      1x21504x1
/sem_seg_head/transformer/encoder/layers.0/norm1/Sqrt                                    Sqrt                   516,096          0.00%       86,016          0.00%       0           0.00%       1x21504x1      1x21504x1
/sem_seg_head/transformer/encoder/layers.0/norm1/Div                                     Div                    22,020,096       0.01%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.0/norm1/Mul                                     Mul                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.0/norm1/Add_1                                   Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.0/linear1/MatMul                                MatMul                 5,637,144,576    2.10%       89,128,960      0.40%       262,144     0.33%       1x21504x256    1x21504x1024
/sem_seg_head/transformer/encoder/layers.0/linear1/Add                                   Add                    22,020,096       0.01%       88,084,480      0.40%       1,024       0.00%       1024           1x21504x1024
/sem_seg_head/transformer/encoder/layers.0/Relu                                          Relu                   22,020,096       0.01%       88,080,384      0.40%       0           0.00%       1x21504x1024   1x21504x1024
/sem_seg_head/transformer/encoder/layers.0/linear2/MatMul                                MatMul                 5,637,144,576    2.10%       23,068,672      0.10%       262,144     0.33%       1x21504x1024   1x21504x256
/sem_seg_head/transformer/encoder/layers.0/linear2/Add                                   Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       256            1x21504x256
/sem_seg_head/transformer/encoder/layers.0/Add_2                                         Add                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.0/norm2/ReduceMean                              ReduceMean             5,505,024        0.00%       86,016          0.00%       0           0.00%       1x21504x256    1x21504x1
/sem_seg_head/transformer/encoder/layers.0/norm2/Sub                                     Sub                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.0/norm2/Pow                                     Pow                    176,160,768      0.07%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.0/norm2/ReduceMean_1                            ReduceMean             5,505,024        0.00%       86,016          0.00%       0           0.00%       1x21504x256    1x21504x1
/sem_seg_head/transformer/encoder/layers.0/norm2/Add                                     Add                    21,504           0.00%       86,016          0.00%       0           0.00%       1x21504x1      1x21504x1
/sem_seg_head/transformer/encoder/layers.0/norm2/Sqrt                                    Sqrt                   516,096          0.00%       86,016          0.00%       0           0.00%       1x21504x1      1x21504x1
/sem_seg_head/transformer/encoder/layers.0/norm2/Div                                     Div                    22,020,096       0.01%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.0/norm2/Mul                                     Mul                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.0/norm2/Add_1                                   Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.1/Add                                           Add                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.1/self_attn/value_proj/MatMul                   MatMul                 1,409,286,144    0.52%       22,282,240      0.10%       65,536      0.08%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.1/self_attn/value_proj/Add                      Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       256            1x21504x256
/sem_seg_head/transformer/encoder/layers.1/self_attn/sampling_offsets/MatMul             MatMul                 1,056,964,608    0.39%       16,711,680      0.08%       49,152      0.06%       1x21504x256    1x21504x192
/sem_seg_head/transformer/encoder/layers.1/self_attn/attention_weights/MatMul            MatMul                 528,482,304      0.20%       8,355,840       0.04%       24,576      0.03%       1x21504x256    1x21504x96
/sem_seg_head/transformer/encoder/layers.1/self_attn/Where                               Where                  0                0.00%       22,020,096      0.10%       0           0.00%       1x21504x1      1x21504x256
/sem_seg_head/transformer/encoder/layers.1/self_attn/sampling_offsets/Add                Add                    4,128,768        0.00%       16,515,840      0.07%       192         0.00%       192            1x21504x192
/sem_seg_head/transformer/encoder/layers.1/self_attn/attention_weights/Add               Add                    2,064,384        0.00%       8,257,920       0.04%       96          0.00%       96             1x21504x96
/sem_seg_head/transformer/encoder/layers.1/self_attn/Div                                 Div                    16,515,072       0.01%       16,515,072      0.07%       0           0.00%       1x21504x192    1x21504x192
/sem_seg_head/transformer/encoder/layers.1/self_attn/Softmax                             Softmax                74,317,824       0.03%       8,257,536       0.04%       0           0.00%       1x21504x8x12   1x21504x8x12
/sem_seg_head/transformer/encoder/layers.1/self_attn/Add                                 Add                    4,128,768        0.00%       16,515,072      0.07%       0           0.00%       1x21504x192    1x21504x192
/sem_seg_head/transformer/encoder/layers.1/self_attn/Split                               Split                  0                0.00%       22,020,096      0.10%       0           0.00%       1x21504x8x32   1x1024x8x32
/sem_seg_head/transformer/encoder/layers.1/self_attn/Mul_2                               Mul                    4,128,768        0.00%       16,515,072      0.07%       0           0.00%       1x21504x192    1x21504x192
/sem_seg_head/transformer/encoder/layers.1/self_attn/Sub                                 Sub                    4,128,768        0.00%       16,515,072      0.07%       0           0.00%       1x21504x192    1x21504x192
/sem_seg_head/transformer/encoder/layers.1/self_attn/Transpose                           Transpose              0                0.00%       1,048,576       0.00%       0           0.00%       1x1024x256     1x256x1024
/sem_seg_head/transformer/encoder/layers.1/self_attn/Transpose_2                         Transpose              0                0.00%       4,194,304       0.02%       0           0.00%       1x4096x256     1x256x4096
/sem_seg_head/transformer/encoder/layers.1/self_attn/Transpose_4                         Transpose              0                0.00%       16,777,216      0.08%       0           0.00%       1x16384x256    1x256x16384
/sem_seg_head/transformer/encoder/layers.1/self_attn/Transpose_6                         Transpose              0                0.00%       8,257,536       0.04%       0           0.00%       1x21504x8x3x4  1x8x21504x3x4
/sem_seg_head/transformer/encoder/layers.1/self_attn/Transpose_1                         Transpose              0                0.00%       5,505,024       0.02%       0           0.00%       1x21504x8x4x2  1x8x21504x4x2
/sem_seg_head/transformer/encoder/layers.1/self_attn/Transpose_3                         Transpose              0                0.00%       5,505,024       0.02%       0           0.00%       1x21504x8x4x2  1x8x21504x4x2
/sem_seg_head/transformer/encoder/layers.1/self_attn/Transpose_5                         Transpose              0                0.00%       5,505,024       0.02%       0           0.00%       1x21504x8x4x2  1x8x21504x4x2
/sem_seg_head/transformer/encoder/layers.1/self_attn/GridSample                          GridSample             0                0.00%       88,080,384      0.40%       0           0.00%       8x32x32x32     8x32x21504x4
/sem_seg_head/transformer/encoder/layers.1/self_attn/GridSample_1                        GridSample             0                0.00%       88,080,384      0.40%       0           0.00%       8x32x64x64     8x32x21504x4
/sem_seg_head/transformer/encoder/layers.1/self_attn/GridSample_2                        GridSample             0                0.00%       88,080,384      0.40%       0           0.00%       8x32x128x128   8x32x21504x4
/sem_seg_head/transformer/encoder/layers.1/self_attn/Mul_3                               Mul                    66,060,288       0.02%       264,241,152     1.19%       0           0.00%       8x32x21504x12  8x32x21504x12
/sem_seg_head/transformer/encoder/layers.1/self_attn/ReduceSum                           ReduceSum              66,060,288       0.02%       22,020,096      0.10%       0           0.00%       8x32x21504x12  8x32x21504
/sem_seg_head/transformer/encoder/layers.1/self_attn/Transpose_7                         Transpose              0                0.00%       22,020,096      0.10%       0           0.00%       1x256x21504    1x21504x256
/sem_seg_head/transformer/encoder/layers.1/self_attn/output_proj/MatMul                  MatMul                 1,409,286,144    0.52%       22,282,240      0.10%       65,536      0.08%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.1/self_attn/output_proj/Add                     Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       256            1x21504x256
/sem_seg_head/transformer/encoder/layers.1/Add_1                                         Add                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.1/norm1/ReduceMean                              ReduceMean             5,505,024        0.00%       86,016          0.00%       0           0.00%       1x21504x256    1x21504x1
/sem_seg_head/transformer/encoder/layers.1/norm1/Sub                                     Sub                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.1/norm1/Pow                                     Pow                    176,160,768      0.07%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.1/norm1/ReduceMean_1                            ReduceMean             5,505,024        0.00%       86,016          0.00%       0           0.00%       1x21504x256    1x21504x1
/sem_seg_head/transformer/encoder/layers.1/norm1/Add                                     Add                    21,504           0.00%       86,016          0.00%       0           0.00%       1x21504x1      1x21504x1
/sem_seg_head/transformer/encoder/layers.1/norm1/Sqrt                                    Sqrt                   516,096          0.00%       86,016          0.00%       0           0.00%       1x21504x1      1x21504x1
/sem_seg_head/transformer/encoder/layers.1/norm1/Div                                     Div                    22,020,096       0.01%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.1/norm1/Mul                                     Mul                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.1/norm1/Add_1                                   Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.1/linear1/MatMul                                MatMul                 5,637,144,576    2.10%       89,128,960      0.40%       262,144     0.33%       1x21504x256    1x21504x1024
/sem_seg_head/transformer/encoder/layers.1/linear1/Add                                   Add                    22,020,096       0.01%       88,084,480      0.40%       1,024       0.00%       1024           1x21504x1024
/sem_seg_head/transformer/encoder/layers.1/Relu                                          Relu                   22,020,096       0.01%       88,080,384      0.40%       0           0.00%       1x21504x1024   1x21504x1024
/sem_seg_head/transformer/encoder/layers.1/linear2/MatMul                                MatMul                 5,637,144,576    2.10%       23,068,672      0.10%       262,144     0.33%       1x21504x1024   1x21504x256
/sem_seg_head/transformer/encoder/layers.1/linear2/Add                                   Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       256            1x21504x256
/sem_seg_head/transformer/encoder/layers.1/Add_2                                         Add                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.1/norm2/ReduceMean                              ReduceMean             5,505,024        0.00%       86,016          0.00%       0           0.00%       1x21504x256    1x21504x1
/sem_seg_head/transformer/encoder/layers.1/norm2/Sub                                     Sub                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.1/norm2/Pow                                     Pow                    176,160,768      0.07%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.1/norm2/ReduceMean_1                            ReduceMean             5,505,024        0.00%       86,016          0.00%       0           0.00%       1x21504x256    1x21504x1
/sem_seg_head/transformer/encoder/layers.1/norm2/Add                                     Add                    21,504           0.00%       86,016          0.00%       0           0.00%       1x21504x1      1x21504x1
/sem_seg_head/transformer/encoder/layers.1/norm2/Sqrt                                    Sqrt                   516,096          0.00%       86,016          0.00%       0           0.00%       1x21504x1      1x21504x1
/sem_seg_head/transformer/encoder/layers.1/norm2/Div                                     Div                    22,020,096       0.01%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.1/norm2/Mul                                     Mul                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.1/norm2/Add_1                                   Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.2/Add                                           Add                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.2/self_attn/value_proj/MatMul                   MatMul                 1,409,286,144    0.52%       22,282,240      0.10%       65,536      0.08%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.2/self_attn/value_proj/Add                      Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       256            1x21504x256
/sem_seg_head/transformer/encoder/layers.2/self_attn/sampling_offsets/MatMul             MatMul                 1,056,964,608    0.39%       16,711,680      0.08%       49,152      0.06%       1x21504x256    1x21504x192
/sem_seg_head/transformer/encoder/layers.2/self_attn/attention_weights/MatMul            MatMul                 528,482,304      0.20%       8,355,840       0.04%       24,576      0.03%       1x21504x256    1x21504x96
/sem_seg_head/transformer/encoder/layers.2/self_attn/Where                               Where                  0                0.00%       22,020,096      0.10%       0           0.00%       1x21504x1      1x21504x256
/sem_seg_head/transformer/encoder/layers.2/self_attn/sampling_offsets/Add                Add                    4,128,768        0.00%       16,515,840      0.07%       192         0.00%       192            1x21504x192
/sem_seg_head/transformer/encoder/layers.2/self_attn/attention_weights/Add               Add                    2,064,384        0.00%       8,257,920       0.04%       96          0.00%       96             1x21504x96
/sem_seg_head/transformer/encoder/layers.2/self_attn/Div                                 Div                    16,515,072       0.01%       16,515,072      0.07%       0           0.00%       1x21504x192    1x21504x192
/sem_seg_head/transformer/encoder/layers.2/self_attn/Softmax                             Softmax                74,317,824       0.03%       8,257,536       0.04%       0           0.00%       1x21504x8x12   1x21504x8x12
/sem_seg_head/transformer/encoder/layers.2/self_attn/Add                                 Add                    4,128,768        0.00%       16,515,072      0.07%       0           0.00%       1x21504x192    1x21504x192
/sem_seg_head/transformer/encoder/layers.2/self_attn/Split                               Split                  0                0.00%       22,020,096      0.10%       0           0.00%       1x21504x8x32   1x1024x8x32
/sem_seg_head/transformer/encoder/layers.2/self_attn/Mul_2                               Mul                    4,128,768        0.00%       16,515,072      0.07%       0           0.00%       1x21504x192    1x21504x192
/sem_seg_head/transformer/encoder/layers.2/self_attn/Sub                                 Sub                    4,128,768        0.00%       16,515,072      0.07%       0           0.00%       1x21504x192    1x21504x192
/sem_seg_head/transformer/encoder/layers.2/self_attn/Transpose                           Transpose              0                0.00%       1,048,576       0.00%       0           0.00%       1x1024x256     1x256x1024
/sem_seg_head/transformer/encoder/layers.2/self_attn/Transpose_2                         Transpose              0                0.00%       4,194,304       0.02%       0           0.00%       1x4096x256     1x256x4096
/sem_seg_head/transformer/encoder/layers.2/self_attn/Transpose_4                         Transpose              0                0.00%       16,777,216      0.08%       0           0.00%       1x16384x256    1x256x16384
/sem_seg_head/transformer/encoder/layers.2/self_attn/Transpose_6                         Transpose              0                0.00%       8,257,536       0.04%       0           0.00%       1x21504x8x3x4  1x8x21504x3x4
/sem_seg_head/transformer/encoder/layers.2/self_attn/Transpose_1                         Transpose              0                0.00%       5,505,024       0.02%       0           0.00%       1x21504x8x4x2  1x8x21504x4x2
/sem_seg_head/transformer/encoder/layers.2/self_attn/Transpose_3                         Transpose              0                0.00%       5,505,024       0.02%       0           0.00%       1x21504x8x4x2  1x8x21504x4x2
/sem_seg_head/transformer/encoder/layers.2/self_attn/Transpose_5                         Transpose              0                0.00%       5,505,024       0.02%       0           0.00%       1x21504x8x4x2  1x8x21504x4x2
/sem_seg_head/transformer/encoder/layers.2/self_attn/GridSample                          GridSample             0                0.00%       88,080,384      0.40%       0           0.00%       8x32x32x32     8x32x21504x4
/sem_seg_head/transformer/encoder/layers.2/self_attn/GridSample_1                        GridSample             0                0.00%       88,080,384      0.40%       0           0.00%       8x32x64x64     8x32x21504x4
/sem_seg_head/transformer/encoder/layers.2/self_attn/GridSample_2                        GridSample             0                0.00%       88,080,384      0.40%       0           0.00%       8x32x128x128   8x32x21504x4
/sem_seg_head/transformer/encoder/layers.2/self_attn/Mul_3                               Mul                    66,060,288       0.02%       264,241,152     1.19%       0           0.00%       8x32x21504x12  8x32x21504x12
/sem_seg_head/transformer/encoder/layers.2/self_attn/ReduceSum                           ReduceSum              66,060,288       0.02%       22,020,096      0.10%       0           0.00%       8x32x21504x12  8x32x21504
/sem_seg_head/transformer/encoder/layers.2/self_attn/Transpose_7                         Transpose              0                0.00%       22,020,096      0.10%       0           0.00%       1x256x21504    1x21504x256
/sem_seg_head/transformer/encoder/layers.2/self_attn/output_proj/MatMul                  MatMul                 1,409,286,144    0.52%       22,282,240      0.10%       65,536      0.08%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.2/self_attn/output_proj/Add                     Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       256            1x21504x256
/sem_seg_head/transformer/encoder/layers.2/Add_1                                         Add                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.2/norm1/ReduceMean                              ReduceMean             5,505,024        0.00%       86,016          0.00%       0           0.00%       1x21504x256    1x21504x1
/sem_seg_head/transformer/encoder/layers.2/norm1/Sub                                     Sub                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.2/norm1/Pow                                     Pow                    176,160,768      0.07%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.2/norm1/ReduceMean_1                            ReduceMean             5,505,024        0.00%       86,016          0.00%       0           0.00%       1x21504x256    1x21504x1
/sem_seg_head/transformer/encoder/layers.2/norm1/Add                                     Add                    21,504           0.00%       86,016          0.00%       0           0.00%       1x21504x1      1x21504x1
/sem_seg_head/transformer/encoder/layers.2/norm1/Sqrt                                    Sqrt                   516,096          0.00%       86,016          0.00%       0           0.00%       1x21504x1      1x21504x1
/sem_seg_head/transformer/encoder/layers.2/norm1/Div                                     Div                    22,020,096       0.01%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.2/norm1/Mul                                     Mul                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.2/norm1/Add_1                                   Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.2/linear1/MatMul                                MatMul                 5,637,144,576    2.10%       89,128,960      0.40%       262,144     0.33%       1x21504x256    1x21504x1024
/sem_seg_head/transformer/encoder/layers.2/linear1/Add                                   Add                    22,020,096       0.01%       88,084,480      0.40%       1,024       0.00%       1024           1x21504x1024
/sem_seg_head/transformer/encoder/layers.2/Relu                                          Relu                   22,020,096       0.01%       88,080,384      0.40%       0           0.00%       1x21504x1024   1x21504x1024
/sem_seg_head/transformer/encoder/layers.2/linear2/MatMul                                MatMul                 5,637,144,576    2.10%       23,068,672      0.10%       262,144     0.33%       1x21504x1024   1x21504x256
/sem_seg_head/transformer/encoder/layers.2/linear2/Add                                   Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       256            1x21504x256
/sem_seg_head/transformer/encoder/layers.2/Add_2                                         Add                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.2/norm2/ReduceMean                              ReduceMean             5,505,024        0.00%       86,016          0.00%       0           0.00%       1x21504x256    1x21504x1
/sem_seg_head/transformer/encoder/layers.2/norm2/Sub                                     Sub                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.2/norm2/Pow                                     Pow                    176,160,768      0.07%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.2/norm2/ReduceMean_1                            ReduceMean             5,505,024        0.00%       86,016          0.00%       0           0.00%       1x21504x256    1x21504x1
/sem_seg_head/transformer/encoder/layers.2/norm2/Add                                     Add                    21,504           0.00%       86,016          0.00%       0           0.00%       1x21504x1      1x21504x1
/sem_seg_head/transformer/encoder/layers.2/norm2/Sqrt                                    Sqrt                   516,096          0.00%       86,016          0.00%       0           0.00%       1x21504x1      1x21504x1
/sem_seg_head/transformer/encoder/layers.2/norm2/Div                                     Div                    22,020,096       0.01%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.2/norm2/Mul                                     Mul                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.2/norm2/Add_1                                   Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.3/Add                                           Add                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.3/self_attn/value_proj/MatMul                   MatMul                 1,409,286,144    0.52%       22,282,240      0.10%       65,536      0.08%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.3/self_attn/value_proj/Add                      Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       256            1x21504x256
/sem_seg_head/transformer/encoder/layers.3/self_attn/sampling_offsets/MatMul             MatMul                 1,056,964,608    0.39%       16,711,680      0.08%       49,152      0.06%       1x21504x256    1x21504x192
/sem_seg_head/transformer/encoder/layers.3/self_attn/attention_weights/MatMul            MatMul                 528,482,304      0.20%       8,355,840       0.04%       24,576      0.03%       1x21504x256    1x21504x96
/sem_seg_head/transformer/encoder/layers.3/self_attn/Where                               Where                  0                0.00%       22,020,096      0.10%       0           0.00%       1x21504x1      1x21504x256
/sem_seg_head/transformer/encoder/layers.3/self_attn/sampling_offsets/Add                Add                    4,128,768        0.00%       16,515,840      0.07%       192         0.00%       192            1x21504x192
/sem_seg_head/transformer/encoder/layers.3/self_attn/attention_weights/Add               Add                    2,064,384        0.00%       8,257,920       0.04%       96          0.00%       96             1x21504x96
/sem_seg_head/transformer/encoder/layers.3/self_attn/Div                                 Div                    16,515,072       0.01%       16,515,072      0.07%       0           0.00%       1x21504x192    1x21504x192
/sem_seg_head/transformer/encoder/layers.3/self_attn/Softmax                             Softmax                74,317,824       0.03%       8,257,536       0.04%       0           0.00%       1x21504x8x12   1x21504x8x12
/sem_seg_head/transformer/encoder/layers.3/self_attn/Add                                 Add                    4,128,768        0.00%       16,515,072      0.07%       0           0.00%       1x21504x192    1x21504x192
/sem_seg_head/transformer/encoder/layers.3/self_attn/Split                               Split                  0                0.00%       22,020,096      0.10%       0           0.00%       1x21504x8x32   1x1024x8x32
/sem_seg_head/transformer/encoder/layers.3/self_attn/Mul_2                               Mul                    4,128,768        0.00%       16,515,072      0.07%       0           0.00%       1x21504x192    1x21504x192
/sem_seg_head/transformer/encoder/layers.3/self_attn/Sub                                 Sub                    4,128,768        0.00%       16,515,072      0.07%       0           0.00%       1x21504x192    1x21504x192
/sem_seg_head/transformer/encoder/layers.3/self_attn/Transpose                           Transpose              0                0.00%       1,048,576       0.00%       0           0.00%       1x1024x256     1x256x1024
/sem_seg_head/transformer/encoder/layers.3/self_attn/Transpose_2                         Transpose              0                0.00%       4,194,304       0.02%       0           0.00%       1x4096x256     1x256x4096
/sem_seg_head/transformer/encoder/layers.3/self_attn/Transpose_4                         Transpose              0                0.00%       16,777,216      0.08%       0           0.00%       1x16384x256    1x256x16384
/sem_seg_head/transformer/encoder/layers.3/self_attn/Transpose_6                         Transpose              0                0.00%       8,257,536       0.04%       0           0.00%       1x21504x8x3x4  1x8x21504x3x4
/sem_seg_head/transformer/encoder/layers.3/self_attn/Transpose_1                         Transpose              0                0.00%       5,505,024       0.02%       0           0.00%       1x21504x8x4x2  1x8x21504x4x2
/sem_seg_head/transformer/encoder/layers.3/self_attn/Transpose_3                         Transpose              0                0.00%       5,505,024       0.02%       0           0.00%       1x21504x8x4x2  1x8x21504x4x2
/sem_seg_head/transformer/encoder/layers.3/self_attn/Transpose_5                         Transpose              0                0.00%       5,505,024       0.02%       0           0.00%       1x21504x8x4x2  1x8x21504x4x2
/sem_seg_head/transformer/encoder/layers.3/self_attn/GridSample                          GridSample             0                0.00%       88,080,384      0.40%       0           0.00%       8x32x32x32     8x32x21504x4
/sem_seg_head/transformer/encoder/layers.3/self_attn/GridSample_1                        GridSample             0                0.00%       88,080,384      0.40%       0           0.00%       8x32x64x64     8x32x21504x4
/sem_seg_head/transformer/encoder/layers.3/self_attn/GridSample_2                        GridSample             0                0.00%       88,080,384      0.40%       0           0.00%       8x32x128x128   8x32x21504x4
/sem_seg_head/transformer/encoder/layers.3/self_attn/Mul_3                               Mul                    66,060,288       0.02%       264,241,152     1.19%       0           0.00%       8x32x21504x12  8x32x21504x12
/sem_seg_head/transformer/encoder/layers.3/self_attn/ReduceSum                           ReduceSum              66,060,288       0.02%       22,020,096      0.10%       0           0.00%       8x32x21504x12  8x32x21504
/sem_seg_head/transformer/encoder/layers.3/self_attn/Transpose_7                         Transpose              0                0.00%       22,020,096      0.10%       0           0.00%       1x256x21504    1x21504x256
/sem_seg_head/transformer/encoder/layers.3/self_attn/output_proj/MatMul                  MatMul                 1,409,286,144    0.52%       22,282,240      0.10%       65,536      0.08%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.3/self_attn/output_proj/Add                     Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       256            1x21504x256
/sem_seg_head/transformer/encoder/layers.3/Add_1                                         Add                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.3/norm1/ReduceMean                              ReduceMean             5,505,024        0.00%       86,016          0.00%       0           0.00%       1x21504x256    1x21504x1
/sem_seg_head/transformer/encoder/layers.3/norm1/Sub                                     Sub                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.3/norm1/Pow                                     Pow                    176,160,768      0.07%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.3/norm1/ReduceMean_1                            ReduceMean             5,505,024        0.00%       86,016          0.00%       0           0.00%       1x21504x256    1x21504x1
/sem_seg_head/transformer/encoder/layers.3/norm1/Add                                     Add                    21,504           0.00%       86,016          0.00%       0           0.00%       1x21504x1      1x21504x1
/sem_seg_head/transformer/encoder/layers.3/norm1/Sqrt                                    Sqrt                   516,096          0.00%       86,016          0.00%       0           0.00%       1x21504x1      1x21504x1
/sem_seg_head/transformer/encoder/layers.3/norm1/Div                                     Div                    22,020,096       0.01%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.3/norm1/Mul                                     Mul                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.3/norm1/Add_1                                   Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.3/linear1/MatMul                                MatMul                 5,637,144,576    2.10%       89,128,960      0.40%       262,144     0.33%       1x21504x256    1x21504x1024
/sem_seg_head/transformer/encoder/layers.3/linear1/Add                                   Add                    22,020,096       0.01%       88,084,480      0.40%       1,024       0.00%       1024           1x21504x1024
/sem_seg_head/transformer/encoder/layers.3/Relu                                          Relu                   22,020,096       0.01%       88,080,384      0.40%       0           0.00%       1x21504x1024   1x21504x1024
/sem_seg_head/transformer/encoder/layers.3/linear2/MatMul                                MatMul                 5,637,144,576    2.10%       23,068,672      0.10%       262,144     0.33%       1x21504x1024   1x21504x256
/sem_seg_head/transformer/encoder/layers.3/linear2/Add                                   Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       256            1x21504x256
/sem_seg_head/transformer/encoder/layers.3/Add_2                                         Add                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.3/norm2/ReduceMean                              ReduceMean             5,505,024        0.00%       86,016          0.00%       0           0.00%       1x21504x256    1x21504x1
/sem_seg_head/transformer/encoder/layers.3/norm2/Sub                                     Sub                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.3/norm2/Pow                                     Pow                    176,160,768      0.07%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.3/norm2/ReduceMean_1                            ReduceMean             5,505,024        0.00%       86,016          0.00%       0           0.00%       1x21504x256    1x21504x1
/sem_seg_head/transformer/encoder/layers.3/norm2/Add                                     Add                    21,504           0.00%       86,016          0.00%       0           0.00%       1x21504x1      1x21504x1
/sem_seg_head/transformer/encoder/layers.3/norm2/Sqrt                                    Sqrt                   516,096          0.00%       86,016          0.00%       0           0.00%       1x21504x1      1x21504x1
/sem_seg_head/transformer/encoder/layers.3/norm2/Div                                     Div                    22,020,096       0.01%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.3/norm2/Mul                                     Mul                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.3/norm2/Add_1                                   Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.4/Add                                           Add                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.4/self_attn/value_proj/MatMul                   MatMul                 1,409,286,144    0.52%       22,282,240      0.10%       65,536      0.08%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.4/self_attn/value_proj/Add                      Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       256            1x21504x256
/sem_seg_head/transformer/encoder/layers.4/self_attn/sampling_offsets/MatMul             MatMul                 1,056,964,608    0.39%       16,711,680      0.08%       49,152      0.06%       1x21504x256    1x21504x192
/sem_seg_head/transformer/encoder/layers.4/self_attn/attention_weights/MatMul            MatMul                 528,482,304      0.20%       8,355,840       0.04%       24,576      0.03%       1x21504x256    1x21504x96
/sem_seg_head/transformer/encoder/layers.4/self_attn/Where                               Where                  0                0.00%       22,020,096      0.10%       0           0.00%       1x21504x1      1x21504x256
/sem_seg_head/transformer/encoder/layers.4/self_attn/sampling_offsets/Add                Add                    4,128,768        0.00%       16,515,840      0.07%       192         0.00%       192            1x21504x192
/sem_seg_head/transformer/encoder/layers.4/self_attn/attention_weights/Add               Add                    2,064,384        0.00%       8,257,920       0.04%       96          0.00%       96             1x21504x96
/sem_seg_head/transformer/encoder/layers.4/self_attn/Div                                 Div                    16,515,072       0.01%       16,515,072      0.07%       0           0.00%       1x21504x192    1x21504x192
/sem_seg_head/transformer/encoder/layers.4/self_attn/Softmax                             Softmax                74,317,824       0.03%       8,257,536       0.04%       0           0.00%       1x21504x8x12   1x21504x8x12
/sem_seg_head/transformer/encoder/layers.4/self_attn/Add                                 Add                    4,128,768        0.00%       16,515,072      0.07%       0           0.00%       1x21504x192    1x21504x192
/sem_seg_head/transformer/encoder/layers.4/self_attn/Split                               Split                  0                0.00%       22,020,096      0.10%       0           0.00%       1x21504x8x32   1x1024x8x32
/sem_seg_head/transformer/encoder/layers.4/self_attn/Mul_2                               Mul                    4,128,768        0.00%       16,515,072      0.07%       0           0.00%       1x21504x192    1x21504x192
/sem_seg_head/transformer/encoder/layers.4/self_attn/Sub                                 Sub                    4,128,768        0.00%       16,515,072      0.07%       0           0.00%       1x21504x192    1x21504x192
/sem_seg_head/transformer/encoder/layers.4/self_attn/Transpose                           Transpose              0                0.00%       1,048,576       0.00%       0           0.00%       1x1024x256     1x256x1024
/sem_seg_head/transformer/encoder/layers.4/self_attn/Transpose_2                         Transpose              0                0.00%       4,194,304       0.02%       0           0.00%       1x4096x256     1x256x4096
/sem_seg_head/transformer/encoder/layers.4/self_attn/Transpose_4                         Transpose              0                0.00%       16,777,216      0.08%       0           0.00%       1x16384x256    1x256x16384
/sem_seg_head/transformer/encoder/layers.4/self_attn/Transpose_6                         Transpose              0                0.00%       8,257,536       0.04%       0           0.00%       1x21504x8x3x4  1x8x21504x3x4
/sem_seg_head/transformer/encoder/layers.4/self_attn/Transpose_1                         Transpose              0                0.00%       5,505,024       0.02%       0           0.00%       1x21504x8x4x2  1x8x21504x4x2
/sem_seg_head/transformer/encoder/layers.4/self_attn/Transpose_3                         Transpose              0                0.00%       5,505,024       0.02%       0           0.00%       1x21504x8x4x2  1x8x21504x4x2
/sem_seg_head/transformer/encoder/layers.4/self_attn/Transpose_5                         Transpose              0                0.00%       5,505,024       0.02%       0           0.00%       1x21504x8x4x2  1x8x21504x4x2
/sem_seg_head/transformer/encoder/layers.4/self_attn/GridSample                          GridSample             0                0.00%       88,080,384      0.40%       0           0.00%       8x32x32x32     8x32x21504x4
/sem_seg_head/transformer/encoder/layers.4/self_attn/GridSample_1                        GridSample             0                0.00%       88,080,384      0.40%       0           0.00%       8x32x64x64     8x32x21504x4
/sem_seg_head/transformer/encoder/layers.4/self_attn/GridSample_2                        GridSample             0                0.00%       88,080,384      0.40%       0           0.00%       8x32x128x128   8x32x21504x4
/sem_seg_head/transformer/encoder/layers.4/self_attn/Mul_3                               Mul                    66,060,288       0.02%       264,241,152     1.19%       0           0.00%       8x32x21504x12  8x32x21504x12
/sem_seg_head/transformer/encoder/layers.4/self_attn/ReduceSum                           ReduceSum              66,060,288       0.02%       22,020,096      0.10%       0           0.00%       8x32x21504x12  8x32x21504
/sem_seg_head/transformer/encoder/layers.4/self_attn/Transpose_7                         Transpose              0                0.00%       22,020,096      0.10%       0           0.00%       1x256x21504    1x21504x256
/sem_seg_head/transformer/encoder/layers.4/self_attn/output_proj/MatMul                  MatMul                 1,409,286,144    0.52%       22,282,240      0.10%       65,536      0.08%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.4/self_attn/output_proj/Add                     Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       256            1x21504x256
/sem_seg_head/transformer/encoder/layers.4/Add_1                                         Add                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.4/norm1/ReduceMean                              ReduceMean             5,505,024        0.00%       86,016          0.00%       0           0.00%       1x21504x256    1x21504x1
/sem_seg_head/transformer/encoder/layers.4/norm1/Sub                                     Sub                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.4/norm1/Pow                                     Pow                    176,160,768      0.07%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.4/norm1/ReduceMean_1                            ReduceMean             5,505,024        0.00%       86,016          0.00%       0           0.00%       1x21504x256    1x21504x1
/sem_seg_head/transformer/encoder/layers.4/norm1/Add                                     Add                    21,504           0.00%       86,016          0.00%       0           0.00%       1x21504x1      1x21504x1
/sem_seg_head/transformer/encoder/layers.4/norm1/Sqrt                                    Sqrt                   516,096          0.00%       86,016          0.00%       0           0.00%       1x21504x1      1x21504x1
/sem_seg_head/transformer/encoder/layers.4/norm1/Div                                     Div                    22,020,096       0.01%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.4/norm1/Mul                                     Mul                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.4/norm1/Add_1                                   Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.4/linear1/MatMul                                MatMul                 5,637,144,576    2.10%       89,128,960      0.40%       262,144     0.33%       1x21504x256    1x21504x1024
/sem_seg_head/transformer/encoder/layers.4/linear1/Add                                   Add                    22,020,096       0.01%       88,084,480      0.40%       1,024       0.00%       1024           1x21504x1024
/sem_seg_head/transformer/encoder/layers.4/Relu                                          Relu                   22,020,096       0.01%       88,080,384      0.40%       0           0.00%       1x21504x1024   1x21504x1024
/sem_seg_head/transformer/encoder/layers.4/linear2/MatMul                                MatMul                 5,637,144,576    2.10%       23,068,672      0.10%       262,144     0.33%       1x21504x1024   1x21504x256
/sem_seg_head/transformer/encoder/layers.4/linear2/Add                                   Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       256            1x21504x256
/sem_seg_head/transformer/encoder/layers.4/Add_2                                         Add                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.4/norm2/ReduceMean                              ReduceMean             5,505,024        0.00%       86,016          0.00%       0           0.00%       1x21504x256    1x21504x1
/sem_seg_head/transformer/encoder/layers.4/norm2/Sub                                     Sub                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.4/norm2/Pow                                     Pow                    176,160,768      0.07%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.4/norm2/ReduceMean_1                            ReduceMean             5,505,024        0.00%       86,016          0.00%       0           0.00%       1x21504x256    1x21504x1
/sem_seg_head/transformer/encoder/layers.4/norm2/Add                                     Add                    21,504           0.00%       86,016          0.00%       0           0.00%       1x21504x1      1x21504x1
/sem_seg_head/transformer/encoder/layers.4/norm2/Sqrt                                    Sqrt                   516,096          0.00%       86,016          0.00%       0           0.00%       1x21504x1      1x21504x1
/sem_seg_head/transformer/encoder/layers.4/norm2/Div                                     Div                    22,020,096       0.01%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.4/norm2/Mul                                     Mul                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.4/norm2/Add_1                                   Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.5/Add                                           Add                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.5/self_attn/value_proj/MatMul                   MatMul                 1,409,286,144    0.52%       22,282,240      0.10%       65,536      0.08%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.5/self_attn/value_proj/Add                      Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       256            1x21504x256
/sem_seg_head/transformer/encoder/layers.5/self_attn/sampling_offsets/MatMul             MatMul                 1,056,964,608    0.39%       16,711,680      0.08%       49,152      0.06%       1x21504x256    1x21504x192
/sem_seg_head/transformer/encoder/layers.5/self_attn/attention_weights/MatMul            MatMul                 528,482,304      0.20%       8,355,840       0.04%       24,576      0.03%       1x21504x256    1x21504x96
/sem_seg_head/transformer/encoder/layers.5/self_attn/Where                               Where                  0                0.00%       22,020,096      0.10%       0           0.00%       1x21504x1      1x21504x256
/sem_seg_head/transformer/encoder/layers.5/self_attn/sampling_offsets/Add                Add                    4,128,768        0.00%       16,515,840      0.07%       192         0.00%       192            1x21504x192
/sem_seg_head/transformer/encoder/layers.5/self_attn/attention_weights/Add               Add                    2,064,384        0.00%       8,257,920       0.04%       96          0.00%       96             1x21504x96
/sem_seg_head/transformer/encoder/layers.5/self_attn/Div                                 Div                    16,515,072       0.01%       16,515,072      0.07%       0           0.00%       1x21504x192    1x21504x192
/sem_seg_head/transformer/encoder/layers.5/self_attn/Softmax                             Softmax                74,317,824       0.03%       8,257,536       0.04%       0           0.00%       1x21504x8x12   1x21504x8x12
/sem_seg_head/transformer/encoder/layers.5/self_attn/Add                                 Add                    4,128,768        0.00%       16,515,072      0.07%       0           0.00%       1x21504x192    1x21504x192
/sem_seg_head/transformer/encoder/layers.5/self_attn/Split                               Split                  0                0.00%       22,020,096      0.10%       0           0.00%       1x21504x8x32   1x1024x8x32
/sem_seg_head/transformer/encoder/layers.5/self_attn/Mul_2                               Mul                    4,128,768        0.00%       16,515,072      0.07%       0           0.00%       1x21504x192    1x21504x192
/sem_seg_head/transformer/encoder/layers.5/self_attn/Sub                                 Sub                    4,128,768        0.00%       16,515,072      0.07%       0           0.00%       1x21504x192    1x21504x192
/sem_seg_head/transformer/encoder/layers.5/self_attn/Transpose                           Transpose              0                0.00%       1,048,576       0.00%       0           0.00%       1x1024x256     1x256x1024
/sem_seg_head/transformer/encoder/layers.5/self_attn/Transpose_2                         Transpose              0                0.00%       4,194,304       0.02%       0           0.00%       1x4096x256     1x256x4096
/sem_seg_head/transformer/encoder/layers.5/self_attn/Transpose_4                         Transpose              0                0.00%       16,777,216      0.08%       0           0.00%       1x16384x256    1x256x16384
/sem_seg_head/transformer/encoder/layers.5/self_attn/Transpose_6                         Transpose              0                0.00%       8,257,536       0.04%       0           0.00%       1x21504x8x3x4  1x8x21504x3x4
/sem_seg_head/transformer/encoder/layers.5/self_attn/Transpose_1                         Transpose              0                0.00%       5,505,024       0.02%       0           0.00%       1x21504x8x4x2  1x8x21504x4x2
/sem_seg_head/transformer/encoder/layers.5/self_attn/Transpose_3                         Transpose              0                0.00%       5,505,024       0.02%       0           0.00%       1x21504x8x4x2  1x8x21504x4x2
/sem_seg_head/transformer/encoder/layers.5/self_attn/Transpose_5                         Transpose              0                0.00%       5,505,024       0.02%       0           0.00%       1x21504x8x4x2  1x8x21504x4x2
/sem_seg_head/transformer/encoder/layers.5/self_attn/GridSample                          GridSample             0                0.00%       88,080,384      0.40%       0           0.00%       8x32x32x32     8x32x21504x4
/sem_seg_head/transformer/encoder/layers.5/self_attn/GridSample_1                        GridSample             0                0.00%       88,080,384      0.40%       0           0.00%       8x32x64x64     8x32x21504x4
/sem_seg_head/transformer/encoder/layers.5/self_attn/GridSample_2                        GridSample             0                0.00%       88,080,384      0.40%       0           0.00%       8x32x128x128   8x32x21504x4
/sem_seg_head/transformer/encoder/layers.5/self_attn/Mul_3                               Mul                    66,060,288       0.02%       264,241,152     1.19%       0           0.00%       8x32x21504x12  8x32x21504x12
/sem_seg_head/transformer/encoder/layers.5/self_attn/ReduceSum                           ReduceSum              66,060,288       0.02%       22,020,096      0.10%       0           0.00%       8x32x21504x12  8x32x21504
/sem_seg_head/transformer/encoder/layers.5/self_attn/Transpose_7                         Transpose              0                0.00%       22,020,096      0.10%       0           0.00%       1x256x21504    1x21504x256
/sem_seg_head/transformer/encoder/layers.5/self_attn/output_proj/MatMul                  MatMul                 1,409,286,144    0.52%       22,282,240      0.10%       65,536      0.08%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.5/self_attn/output_proj/Add                     Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       256            1x21504x256
/sem_seg_head/transformer/encoder/layers.5/Add_1                                         Add                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.5/norm1/ReduceMean                              ReduceMean             5,505,024        0.00%       86,016          0.00%       0           0.00%       1x21504x256    1x21504x1
/sem_seg_head/transformer/encoder/layers.5/norm1/Sub                                     Sub                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.5/norm1/Pow                                     Pow                    176,160,768      0.07%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.5/norm1/ReduceMean_1                            ReduceMean             5,505,024        0.00%       86,016          0.00%       0           0.00%       1x21504x256    1x21504x1
/sem_seg_head/transformer/encoder/layers.5/norm1/Add                                     Add                    21,504           0.00%       86,016          0.00%       0           0.00%       1x21504x1      1x21504x1
/sem_seg_head/transformer/encoder/layers.5/norm1/Sqrt                                    Sqrt                   516,096          0.00%       86,016          0.00%       0           0.00%       1x21504x1      1x21504x1
/sem_seg_head/transformer/encoder/layers.5/norm1/Div                                     Div                    22,020,096       0.01%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.5/norm1/Mul                                     Mul                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.5/norm1/Add_1                                   Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.5/linear1/MatMul                                MatMul                 5,637,144,576    2.10%       89,128,960      0.40%       262,144     0.33%       1x21504x256    1x21504x1024
/sem_seg_head/transformer/encoder/layers.5/linear1/Add                                   Add                    22,020,096       0.01%       88,084,480      0.40%       1,024       0.00%       1024           1x21504x1024
/sem_seg_head/transformer/encoder/layers.5/Relu                                          Relu                   22,020,096       0.01%       88,080,384      0.40%       0           0.00%       1x21504x1024   1x21504x1024
/sem_seg_head/transformer/encoder/layers.5/linear2/MatMul                                MatMul                 5,637,144,576    2.10%       23,068,672      0.10%       262,144     0.33%       1x21504x1024   1x21504x256
/sem_seg_head/transformer/encoder/layers.5/linear2/Add                                   Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       256            1x21504x256
/sem_seg_head/transformer/encoder/layers.5/Add_2                                         Add                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.5/norm2/ReduceMean                              ReduceMean             5,505,024        0.00%       86,016          0.00%       0           0.00%       1x21504x256    1x21504x1
/sem_seg_head/transformer/encoder/layers.5/norm2/Sub                                     Sub                    5,505,024        0.00%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.5/norm2/Pow                                     Pow                    176,160,768      0.07%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.5/norm2/ReduceMean_1                            ReduceMean             5,505,024        0.00%       86,016          0.00%       0           0.00%       1x21504x256    1x21504x1
/sem_seg_head/transformer/encoder/layers.5/norm2/Add                                     Add                    21,504           0.00%       86,016          0.00%       0           0.00%       1x21504x1      1x21504x1
/sem_seg_head/transformer/encoder/layers.5/norm2/Sqrt                                    Sqrt                   516,096          0.00%       86,016          0.00%       0           0.00%       1x21504x1      1x21504x1
/sem_seg_head/transformer/encoder/layers.5/norm2/Div                                     Div                    22,020,096       0.01%       22,020,096      0.10%       0           0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.5/norm2/Mul                                     Mul                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       1x21504x256    1x21504x256
/sem_seg_head/transformer/encoder/layers.5/norm2/Add_1                                   Add                    5,505,024        0.00%       22,021,120      0.10%       256         0.00%       1x21504x256    1x21504x256
/sem_seg_head/Transpose                                                                  Transpose              0                0.00%       1,048,576       0.00%       0           0.00%       1x1024x256     1x256x1024
/sem_seg_head/Transpose_1                                                                Transpose              0                0.00%       4,194,304       0.02%       0           0.00%       1x4096x256     1x256x4096
/sem_seg_head/Transpose_2                                                                Transpose              0                0.00%       16,777,216      0.08%       0           0.00%       1x16384x256    1x256x16384
/sem_seg_head/predictor/Add                                                              Add                    262,144          0.00%       1,049,600       0.00%       256         0.00%       1x256x1024     1x256x1024
/sem_seg_head/predictor/Add_1                                                            Add                    1,048,576        0.00%       4,195,328       0.02%       256         0.00%       1x256x4096     1x256x4096
/sem_seg_head/Resize                                                                     Resize                 67,108,864       0.02%       67,108,864      0.30%       0           0.00%       1x256x128x128  1x256x256x256
/sem_seg_head/predictor/transformer_cross_attention_layers.0/Add_1                       Add                    262,144          0.00%       2,097,152       0.01%       262,144     0.33%       1x256x1024     1x256x1024
/sem_seg_head/predictor/transformer_cross_attention_layers.0/Transpose_1                 Transpose              0                0.00%       1,048,576       0.00%       0           0.00%       1x256x1024     1024x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.1/Add_1                       Add                    1,048,576        0.00%       8,388,608       0.04%       1,048,576   1.30%       1x256x4096     1x256x4096
/sem_seg_head/predictor/transformer_cross_attention_layers.1/Transpose_1                 Transpose              0                0.00%       4,194,304       0.02%       0           0.00%       1x256x4096     4096x1x256
/sem_seg_head/Add                                                                        Add                    16,777,216       0.01%       67,108,864      0.30%       0           0.00%       1x256x256x256  1x256x256x256
/sem_seg_head/predictor/Add_2                                                            Add                    4,194,304        0.00%       16,778,240      0.08%       256         0.00%       1x256x16384    1x256x16384
/sem_seg_head/predictor/transformer_cross_attention_layers.0/Transpose                   Transpose              0                0.00%       1,048,576       0.00%       0           0.00%       1x256x1024     1024x1x256
MatMul_657                                                                               MatMul                 201,326,592      0.07%       3,932,160       0.02%       196,608     0.24%       1024x1x256     1024x1x768
/sem_seg_head/predictor/transformer_cross_attention_layers.1/Transpose                   Transpose              0                0.00%       4,194,304       0.02%       0           0.00%       1x256x4096     4096x1x256
MatMul_772                                                                               MatMul                 805,306,368      0.30%       13,369,344      0.06%       196,608     0.24%       4096x1x256     4096x1x768
/sem_seg_head/layer_1/Conv                                                               Conv                   38,654,705,664   14.39%      69,468,160      0.31%       589,824     0.73%       1x256x256x256  1x256x256x256
MatMul_654                                                                               MatMul                 201,326,592      0.07%       3,932,160       0.02%       196,608     0.24%       1024x1x256     1024x1x768
Split_658                                                                                Split                  0                0.00%       3,145,752       0.01%       3           0.00%       1024x1x768     1024x1x256
MatMul_769                                                                               MatMul                 805,306,368      0.30%       13,369,344      0.06%       196,608     0.24%       4096x1x256     4096x1x768
Split_773                                                                                Split                  0                0.00%       12,582,912      0.06%       0           0.00%       4096x1x768     4096x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.2/Add_1                       Add                    4,194,304        0.00%       33,554,432      0.15%       4,194,304   5.21%       1x256x16384    1x256x16384
/sem_seg_head/predictor/transformer_cross_attention_layers.2/Transpose_1                 Transpose              0                0.00%       16,777,216      0.08%       0           0.00%       1x256x16384    16384x1x256
Split_655                                                                                Split                  0                0.00%       3,145,728       0.01%       0           0.00%       1024x1x768     1024x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.0/multihead_attn/Add_2        Add                    262,144          0.00%       1,049,600       0.00%       256         0.00%       256            1024x1x256
Split_770                                                                                Split                  0                0.00%       12,582,912      0.06%       0           0.00%       4096x1x768     4096x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.1/multihead_attn/Add_2        Add                    1,048,576        0.00%       4,195,328       0.02%       256         0.00%       256            4096x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.2/Transpose                   Transpose              0                0.00%       16,777,216      0.08%       0           0.00%       1x256x16384    16384x1x256
MatMul_892                                                                               MatMul                 3,221,225,472    1.20%       51,118,080      0.23%       196,608     0.24%       16384x1x256    16384x1x768
/sem_seg_head/predictor/transformer_cross_attention_layers.3/multihead_attn/Add_2        Add                    262,144          0.00%       1,049,600       0.00%       256         0.00%       256            1024x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.4/multihead_attn/Add_2        Add                    1,048,576        0.00%       4,195,328       0.02%       256         0.00%       256            4096x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.6/multihead_attn/Add_2        Add                    262,144          0.00%       1,049,600       0.00%       256         0.00%       256            1024x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.7/multihead_attn/Add_2        Add                    1,048,576        0.00%       4,195,328       0.02%       256         0.00%       256            4096x1x256
/sem_seg_head/layer_1/norm/InstanceNormalization                                         InstanceNormalization  117,440,512      0.04%       67,108,864      0.30%       0           0.00%       1x32x524288    1x32x524288
/sem_seg_head/predictor/transformer_cross_attention_layers.0/multihead_attn/Add_1        Add                    262,144          0.00%       1,049,600       0.00%       256         0.00%       256            1024x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.1/multihead_attn/Add_1        Add                    1,048,576        0.00%       4,195,328       0.02%       256         0.00%       256            4096x1x256
MatMul_889                                                                               MatMul                 3,221,225,472    1.20%       51,118,080      0.23%       196,608     0.24%       16384x1x256    16384x1x768
Split_893                                                                                Split                  0                0.00%       50,331,648      0.23%       0           0.00%       16384x1x768    16384x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.3/multihead_attn/Add_1        Add                    262,144          0.00%       1,049,600       0.00%       256         0.00%       256            1024x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.4/multihead_attn/Add_1        Add                    1,048,576        0.00%       4,195,328       0.02%       256         0.00%       256            4096x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.6/multihead_attn/Add_1        Add                    262,144          0.00%       1,049,600       0.00%       256         0.00%       256            1024x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.7/multihead_attn/Add_1        Add                    1,048,576        0.00%       4,195,328       0.02%       256         0.00%       256            4096x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.0/multihead_attn/Transpose_1  Transpose              0                0.00%       1,048,576       0.00%       0           0.00%       1024x8x32      8x1024x32
/sem_seg_head/predictor/transformer_cross_attention_layers.1/multihead_attn/Transpose_1  Transpose              0                0.00%       4,194,304       0.02%       0           0.00%       4096x8x32      8x4096x32
Split_890                                                                                Split                  0                0.00%       50,331,648      0.23%       0           0.00%       16384x1x768    16384x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.2/multihead_attn/Add_2        Add                    4,194,304        0.00%       16,778,240      0.08%       256         0.00%       256            16384x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.3/multihead_attn/Transpose_1  Transpose              0                0.00%       1,048,576       0.00%       0           0.00%       1024x8x32      8x1024x32
/sem_seg_head/predictor/transformer_cross_attention_layers.4/multihead_attn/Transpose_1  Transpose              0                0.00%       4,194,304       0.02%       0           0.00%       4096x8x32      8x4096x32
/sem_seg_head/predictor/transformer_cross_attention_layers.5/multihead_attn/Add_2        Add                    4,194,304        0.00%       16,778,240      0.08%       256         0.00%       256            16384x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.6/multihead_attn/Transpose_1  Transpose              0                0.00%       1,048,576       0.00%       0           0.00%       1024x8x32      8x1024x32
/sem_seg_head/predictor/transformer_cross_attention_layers.7/multihead_attn/Transpose_1  Transpose              0                0.00%       4,194,304       0.02%       0           0.00%       4096x8x32      8x4096x32
/sem_seg_head/predictor/transformer_cross_attention_layers.8/multihead_attn/Add_2        Add                    4,194,304        0.00%       16,778,240      0.08%       256         0.00%       256            16384x1x256
/sem_seg_head/layer_1/norm/Mul                                                           Mul                    16,777,216       0.01%       67,109,888      0.30%       256         0.00%       1x256x256x256  1x256x256x256
/sem_seg_head/predictor/transformer_cross_attention_layers.0/multihead_attn/Transpose_2  Transpose              0                0.00%       1,048,576       0.00%       0           0.00%       1024x8x32      8x32x1024
/sem_seg_head/predictor/transformer_cross_attention_layers.1/multihead_attn/Transpose_2  Transpose              0                0.00%       4,194,304       0.02%       0           0.00%       4096x8x32      8x32x4096
/sem_seg_head/predictor/transformer_cross_attention_layers.2/multihead_attn/Add_1        Add                    4,194,304        0.00%       16,778,240      0.08%       256         0.00%       256            16384x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.3/multihead_attn/Transpose_2  Transpose              0                0.00%       1,048,576       0.00%       0           0.00%       1024x8x32      8x32x1024
/sem_seg_head/predictor/transformer_cross_attention_layers.4/multihead_attn/Transpose_2  Transpose              0                0.00%       4,194,304       0.02%       0           0.00%       4096x8x32      8x32x4096
/sem_seg_head/predictor/transformer_cross_attention_layers.5/multihead_attn/Add_1        Add                    4,194,304        0.00%       16,778,240      0.08%       256         0.00%       256            16384x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.6/multihead_attn/Transpose_2  Transpose              0                0.00%       1,048,576       0.00%       0           0.00%       1024x8x32      8x32x1024
/sem_seg_head/predictor/transformer_cross_attention_layers.7/multihead_attn/Transpose_2  Transpose              0                0.00%       4,194,304       0.02%       0           0.00%       4096x8x32      8x32x4096
/sem_seg_head/predictor/transformer_cross_attention_layers.8/multihead_attn/Add_1        Add                    4,194,304        0.00%       16,778,240      0.08%       256         0.00%       256            16384x1x256
/sem_seg_head/layer_1/norm/Add                                                           Add                    16,777,216       0.01%       67,109,888      0.30%       256         0.00%       1x256x256x256  1x256x256x256
/sem_seg_head/predictor/transformer_cross_attention_layers.0/multihead_attn/MatMul_3     MatMul                 26,214,400       0.01%       3,379,200       0.02%       25,600      0.03%       8x100x32       8x100x1024
/sem_seg_head/predictor/transformer_cross_attention_layers.2/multihead_attn/Transpose_1  Transpose              0                0.00%       16,777,216      0.08%       0           0.00%       16384x8x32     8x16384x32
/sem_seg_head/predictor/transformer_cross_attention_layers.5/multihead_attn/Transpose_1  Transpose              0                0.00%       16,777,216      0.08%       0           0.00%       16384x8x32     8x16384x32
/sem_seg_head/predictor/transformer_cross_attention_layers.8/multihead_attn/Transpose_1  Transpose              0                0.00%       16,777,216      0.08%       0           0.00%       16384x8x32     8x16384x32
/sem_seg_head/layer_1/Relu                                                               Relu                   16,777,216       0.01%       67,108,864      0.30%       0           0.00%       1x256x256x256  1x256x256x256
/sem_seg_head/predictor/transformer_cross_attention_layers.2/multihead_attn/Transpose_2  Transpose              0                0.00%       16,777,216      0.08%       0           0.00%       16384x8x32     8x32x16384
/sem_seg_head/predictor/transformer_cross_attention_layers.5/multihead_attn/Transpose_2  Transpose              0                0.00%       16,777,216      0.08%       0           0.00%       16384x8x32     8x32x16384
/sem_seg_head/predictor/transformer_cross_attention_layers.8/multihead_attn/Transpose_2  Transpose              0                0.00%       16,777,216      0.08%       0           0.00%       16384x8x32     8x32x16384
/sem_seg_head/mask_features/Conv                                                         Conv                   4,311,744,512    1.61%       67,372,032      0.30%       65,792      0.08%       1x256x256x256  1x256x256x256
/sem_seg_head/predictor/MatMul                                                           MatMul                 1,677,721,600    0.62%       26,316,800      0.12%       25,600      0.03%       1x100x256      1x100x65536
/sem_seg_head/predictor/Resize                                                           Resize                 409,600          0.00%       409,632         0.00%       4           0.00%       1x100x256x256  1x100x32x32
/sem_seg_head/predictor/Sigmoid                                                          Sigmoid                3,276,800        0.00%       409,600         0.00%       0           0.00%       1x100x32x32    1x100x32x32
/sem_seg_head/predictor/Tile_2                                                           Tile                   0                0.00%       3,276,832       0.01%       4           0.00%       1x1x100x1024   1x8x100x1024
/sem_seg_head/predictor/Less                                                             Less                   819,200          0.00%       819,200         0.00%       0           0.00%       8x100x1024     8x100x1024
/sem_seg_head/predictor/ReduceSum                                                        ReduceSum              819,200          0.00%       6,400           0.00%       0           0.00%       8x100x1024     8x100
/sem_seg_head/predictor/Equal                                                            Equal                  800              0.00%       800             0.00%       0           0.00%       8x100          8x100
/sem_seg_head/predictor/Sub                                                              Sub                    800              0.00%       3,200           0.00%       0           0.00%       0              8x100
/sem_seg_head/predictor/Mul                                                              Mul                    819,200          0.00%       3,276,800       0.01%       0           0.00%       8x100x1024     8x100x1024
/sem_seg_head/predictor/transformer_cross_attention_layers.0/multihead_attn/Where        Where                  0                0.00%       6,553,600       0.03%       819,200     1.02%       8x100x1024     8x100x1024
/sem_seg_head/predictor/transformer_cross_attention_layers.0/multihead_attn/Add_3        Add                    819,200          0.00%       3,276,800       0.01%       0           0.00%       8x100x1024     8x100x1024
/sem_seg_head/predictor/transformer_cross_attention_layers.0/multihead_attn/Softmax      Softmax                29,491,200       0.01%       3,276,800       0.01%       0           0.00%       8x100x1024     8x100x1024
/sem_seg_head/predictor/transformer_cross_attention_layers.0/multihead_attn/MatMul_4     MatMul                 26,214,400       0.01%       102,400         0.00%       0           0.00%       8x100x1024     8x100x32
/sem_seg_head/predictor/transformer_cross_attention_layers.0/multihead_attn/Transpose_3  Transpose              0                0.00%       102,400         0.00%       0           0.00%       8x100x32       100x8x32
/sem_seg_head/predictor/transformer_cross_attention_layers.0/multihead_attn/Gemm         Gemm                   6,579,200        0.00%       365,568         0.00%       65,792      0.08%       100x256        100x256
/sem_seg_head/predictor/transformer_cross_attention_layers.0/Add_2                       Add                    25,600           0.00%       204,800         0.00%       25,600      0.03%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.0/norm/ReduceMean             ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.0/norm/Sub                    Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.0/norm/Pow                    Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.0/norm/ReduceMean_1           ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.0/norm/Add                    Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.0/norm/Sqrt                   Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.0/norm/Div                    Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.0/norm/Mul                    Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.0/norm/Add_1                  Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.0/Add                          Add                    25,600           0.00%       204,800         0.00%       25,600      0.03%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.0/self_attn/MatMul_2           MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.0/self_attn/MatMul             MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.0/self_attn/MatMul_1           MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.0/self_attn/Add_2              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.0/self_attn/Add                Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.0/self_attn/Add_1              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.0/self_attn/Transpose_1        Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.0/self_attn/Transpose          Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.0/self_attn/Transpose_2        Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x32x100
/sem_seg_head/predictor/transformer_self_attention_layers.0/self_attn/Div                Div                    102,400          0.00%       102,400         0.00%       0           0.00%       8x100x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.0/self_attn/MatMul_3           MatMul                 2,560,000        0.00%       320,000         0.00%       0           0.00%       8x100x32       8x100x100
/sem_seg_head/predictor/transformer_self_attention_layers.0/self_attn/Softmax            Softmax                2,880,000        0.00%       320,000         0.00%       0           0.00%       8x100x100      8x100x100
/sem_seg_head/predictor/transformer_self_attention_layers.0/self_attn/MatMul_4           MatMul                 2,560,000        0.00%       102,400         0.00%       0           0.00%       8x100x100      8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.0/self_attn/Transpose_3        Transpose              0                0.00%       102,400         0.00%       0           0.00%       8x100x32       100x8x32
/sem_seg_head/predictor/transformer_self_attention_layers.0/self_attn/Gemm               Gemm                   6,579,200        0.00%       365,568         0.00%       65,792      0.08%       100x256        100x256
/sem_seg_head/predictor/transformer_self_attention_layers.0/Add_1                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.0/norm/ReduceMean              ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.0/norm/Sub                     Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.0/norm/Pow                     Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.0/norm/ReduceMean_1            ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.0/norm/Add                     Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.0/norm/Sqrt                    Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.0/norm/Div                     Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.0/norm/Mul                     Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.0/norm/Add_1                   Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.0/linear1/MatMul                          MatMul                 52,428,800       0.02%       2,916,352       0.01%       524,288     0.65%       100x1x256      100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.0/linear1/Add                             Add                    204,800          0.00%       827,392         0.00%       2,048       0.00%       2048           100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.0/Relu                                    Relu                   204,800          0.00%       819,200         0.00%       0           0.00%       100x1x2048     100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.0/linear2/MatMul                          MatMul                 52,428,800       0.02%       2,199,552       0.01%       524,288     0.65%       100x1x2048     100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.0/linear2/Add                             Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.0/Add                                     Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.0/norm/ReduceMean                         ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.0/norm/Sub                                Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.0/norm/Pow                                Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.0/norm/ReduceMean_1                       ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.0/norm/Add                                Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.0/norm/Sqrt                               Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.0/norm/Div                                Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.0/norm/Mul                                Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.0/norm/Add_1                              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_1/ReduceMean                                        ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.1/Add                         Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_1/Sub                                               Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.1/multihead_attn/MatMul       MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_1/Pow                                               Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.1/multihead_attn/Add          Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/decoder_norm_1/ReduceMean_1                                      ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/decoder_norm_1/Add                                               Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.1/multihead_attn/Transpose    Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/decoder_norm_1/Sqrt                                              Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.1/multihead_attn/Div          Div                    102,400          0.00%       102,400         0.00%       0           0.00%       8x100x32       8x100x32
/sem_seg_head/predictor/decoder_norm_1/Div                                               Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.1/multihead_attn/MatMul_3     MatMul                 104,857,600      0.04%       13,107,200      0.06%       0           0.00%       8x100x32       8x100x4096
/sem_seg_head/predictor/decoder_norm_1/Mul                                               Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_1/Add_1                                             Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/Transpose_1                                                      Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x1x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.0_1/MatMul                                     MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.0_1/Add                                        Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            1x100x256
/sem_seg_head/predictor/mask_embed_1/Relu                                                Relu                   25,600           0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.1_1/MatMul                                     MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.1_1/Add                                        Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            1x100x256
/sem_seg_head/predictor/mask_embed_1/Relu_1                                              Relu                   25,600           0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.2_1/MatMul                                     MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.2_1/Add                                        Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            1x100x256
/sem_seg_head/predictor/MatMul_1                                                         MatMul                 1,677,721,600    0.62%       26,214,400      0.12%       0           0.00%       1x100x256      1x100x65536
/sem_seg_head/predictor/Resize_1                                                         Resize                 1,638,400        0.00%       1,638,432       0.01%       4           0.00%       1x100x256x256  1x100x64x64
/sem_seg_head/predictor/Sigmoid_1                                                        Sigmoid                13,107,200       0.00%       1,638,400       0.01%       0           0.00%       1x100x64x64    1x100x64x64
/sem_seg_head/predictor/Tile_3                                                           Tile                   0                0.00%       13,107,200      0.06%       0           0.00%       1x1x100x4096   1x8x100x4096
/sem_seg_head/predictor/Less_1                                                           Less                   3,276,800        0.00%       3,276,800       0.01%       0           0.00%       8x100x4096     8x100x4096
/sem_seg_head/predictor/ReduceSum_1                                                      ReduceSum              3,276,800        0.00%       6,400           0.00%       0           0.00%       8x100x4096     8x100
/sem_seg_head/predictor/Equal_1                                                          Equal                  800              0.00%       800             0.00%       0           0.00%       8x100          8x100
/sem_seg_head/predictor/Sub_1                                                            Sub                    800              0.00%       3,200           0.00%       0           0.00%       0              8x100
/sem_seg_head/predictor/Mul_1                                                            Mul                    3,276,800        0.00%       13,107,200      0.06%       0           0.00%       8x100x4096     8x100x4096
/sem_seg_head/predictor/transformer_cross_attention_layers.1/multihead_attn/Where        Where                  0                0.00%       26,214,400      0.12%       3,276,800   4.07%       8x100x4096     8x100x4096
/sem_seg_head/predictor/transformer_cross_attention_layers.1/multihead_attn/Add_3        Add                    3,276,800        0.00%       13,107,200      0.06%       0           0.00%       8x100x4096     8x100x4096
/sem_seg_head/predictor/transformer_cross_attention_layers.1/multihead_attn/Softmax      Softmax                117,964,800      0.04%       13,107,200      0.06%       0           0.00%       8x100x4096     8x100x4096
/sem_seg_head/predictor/transformer_cross_attention_layers.1/multihead_attn/MatMul_4     MatMul                 104,857,600      0.04%       102,400         0.00%       0           0.00%       8x100x4096     8x100x32
/sem_seg_head/predictor/transformer_cross_attention_layers.1/multihead_attn/Transpose_3  Transpose              0                0.00%       102,400         0.00%       0           0.00%       8x100x32       100x8x32
/sem_seg_head/predictor/transformer_cross_attention_layers.1/multihead_attn/Gemm         Gemm                   6,579,200        0.00%       365,568         0.00%       65,792      0.08%       100x256        100x256
/sem_seg_head/predictor/transformer_cross_attention_layers.1/Add_2                       Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.1/norm/ReduceMean             ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.1/norm/Sub                    Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.1/norm/Pow                    Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.1/norm/ReduceMean_1           ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.1/norm/Add                    Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.1/norm/Sqrt                   Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.1/norm/Div                    Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.1/norm/Mul                    Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.1/norm/Add_1                  Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.1/Add                          Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.1/self_attn/MatMul_2           MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.1/self_attn/MatMul             MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.1/self_attn/MatMul_1           MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.1/self_attn/Add_2              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.1/self_attn/Add                Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.1/self_attn/Add_1              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.1/self_attn/Transpose_1        Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.1/self_attn/Transpose          Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.1/self_attn/Transpose_2        Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x32x100
/sem_seg_head/predictor/transformer_self_attention_layers.1/self_attn/Div                Div                    102,400          0.00%       102,400         0.00%       0           0.00%       8x100x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.1/self_attn/MatMul_3           MatMul                 2,560,000        0.00%       320,000         0.00%       0           0.00%       8x100x32       8x100x100
/sem_seg_head/predictor/transformer_self_attention_layers.1/self_attn/Softmax            Softmax                2,880,000        0.00%       320,000         0.00%       0           0.00%       8x100x100      8x100x100
/sem_seg_head/predictor/transformer_self_attention_layers.1/self_attn/MatMul_4           MatMul                 2,560,000        0.00%       102,400         0.00%       0           0.00%       8x100x100      8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.1/self_attn/Transpose_3        Transpose              0                0.00%       102,400         0.00%       0           0.00%       8x100x32       100x8x32
/sem_seg_head/predictor/transformer_self_attention_layers.1/self_attn/Gemm               Gemm                   6,579,200        0.00%       365,568         0.00%       65,792      0.08%       100x256        100x256
/sem_seg_head/predictor/transformer_self_attention_layers.1/Add_1                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.1/norm/ReduceMean              ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.1/norm/Sub                     Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.1/norm/Pow                     Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.1/norm/ReduceMean_1            ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.1/norm/Add                     Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.1/norm/Sqrt                    Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.1/norm/Div                     Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.1/norm/Mul                     Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.1/norm/Add_1                   Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.1/linear1/MatMul                          MatMul                 52,428,800       0.02%       2,916,352       0.01%       524,288     0.65%       100x1x256      100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.1/linear1/Add                             Add                    204,800          0.00%       827,392         0.00%       2,048       0.00%       2048           100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.1/Relu                                    Relu                   204,800          0.00%       819,200         0.00%       0           0.00%       100x1x2048     100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.1/linear2/MatMul                          MatMul                 52,428,800       0.02%       2,199,552       0.01%       524,288     0.65%       100x1x2048     100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.1/linear2/Add                             Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.1/Add                                     Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.1/norm/ReduceMean                         ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.1/norm/Sub                                Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.1/norm/Pow                                Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.1/norm/ReduceMean_1                       ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.1/norm/Add                                Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.1/norm/Sqrt                               Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.1/norm/Div                                Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.1/norm/Mul                                Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.1/norm/Add_1                              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_2/ReduceMean                                        ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.2/Add                         Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_2/Sub                                               Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.2/multihead_attn/MatMul       MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_2/Pow                                               Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.2/multihead_attn/Add          Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/decoder_norm_2/ReduceMean_1                                      ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/decoder_norm_2/Add                                               Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.2/multihead_attn/Transpose    Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/decoder_norm_2/Sqrt                                              Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.2/multihead_attn/Div          Div                    102,400          0.00%       102,400         0.00%       0           0.00%       8x100x32       8x100x32
/sem_seg_head/predictor/decoder_norm_2/Div                                               Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.2/multihead_attn/MatMul_3     MatMul                 419,430,400      0.16%       52,428,800      0.24%       0           0.00%       8x100x32       8x100x16384
/sem_seg_head/predictor/decoder_norm_2/Mul                                               Mul                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_2/Add_1                                             Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/Transpose_2                                                      Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x1x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.0_2/MatMul                                     MatMul                 6,553,600        0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.0_2/Add                                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       256            1x100x256
/sem_seg_head/predictor/mask_embed_2/Relu                                                Relu                   25,600           0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.1_2/MatMul                                     MatMul                 6,553,600        0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.1_2/Add                                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       256            1x100x256
/sem_seg_head/predictor/mask_embed_2/Relu_1                                              Relu                   25,600           0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.2_2/MatMul                                     MatMul                 6,553,600        0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.2_2/Add                                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       256            1x100x256
/sem_seg_head/predictor/MatMul_2                                                         MatMul                 1,677,721,600    0.62%       26,214,400      0.12%       0           0.00%       1x100x256      1x100x65536
/sem_seg_head/predictor/Resize_2                                                         Resize                 6,553,600        0.00%       6,553,632       0.03%       4           0.00%       1x100x256x256  1x100x128x128
/sem_seg_head/predictor/Sigmoid_2                                                        Sigmoid                52,428,800       0.02%       6,553,600       0.03%       0           0.00%       1x100x128x128  1x100x128x128
/sem_seg_head/predictor/Tile_4                                                           Tile                   0                0.00%       52,428,800      0.24%       0           0.00%       1x1x100x16384  1x8x100x16384
/sem_seg_head/predictor/Less_2                                                           Less                   13,107,200       0.00%       13,107,200      0.06%       0           0.00%       8x100x16384    8x100x16384
/sem_seg_head/predictor/ReduceSum_2                                                      ReduceSum              13,107,200       0.00%       6,400           0.00%       0           0.00%       8x100x16384    8x100
/sem_seg_head/predictor/Equal_2                                                          Equal                  800              0.00%       800             0.00%       0           0.00%       8x100          8x100
/sem_seg_head/predictor/Sub_2                                                            Sub                    800              0.00%       3,200           0.00%       0           0.00%       0              8x100
/sem_seg_head/predictor/Mul_2                                                            Mul                    13,107,200       0.00%       52,428,800      0.24%       0           0.00%       8x100x16384    8x100x16384
/sem_seg_head/predictor/transformer_cross_attention_layers.2/multihead_attn/Where        Where                  0                0.00%       104,857,600     0.47%       13,107,200  16.29%      8x100x16384    8x100x16384
/sem_seg_head/predictor/transformer_cross_attention_layers.2/multihead_attn/Add_3        Add                    13,107,200       0.00%       52,428,800      0.24%       0           0.00%       8x100x16384    8x100x16384
/sem_seg_head/predictor/transformer_cross_attention_layers.2/multihead_attn/Softmax      Softmax                471,859,200      0.18%       52,428,800      0.24%       0           0.00%       8x100x16384    8x100x16384
/sem_seg_head/predictor/transformer_cross_attention_layers.2/multihead_attn/MatMul_4     MatMul                 419,430,400      0.16%       102,400         0.00%       0           0.00%       8x100x16384    8x100x32
/sem_seg_head/predictor/transformer_cross_attention_layers.2/multihead_attn/Transpose_3  Transpose              0                0.00%       102,400         0.00%       0           0.00%       8x100x32       100x8x32
/sem_seg_head/predictor/transformer_cross_attention_layers.2/multihead_attn/Gemm         Gemm                   6,579,200        0.00%       365,568         0.00%       65,792      0.08%       100x256        100x256
/sem_seg_head/predictor/transformer_cross_attention_layers.2/Add_2                       Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.2/norm/ReduceMean             ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.2/norm/Sub                    Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.2/norm/Pow                    Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.2/norm/ReduceMean_1           ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.2/norm/Add                    Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.2/norm/Sqrt                   Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.2/norm/Div                    Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.2/norm/Mul                    Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.2/norm/Add_1                  Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.2/Add                          Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.2/self_attn/MatMul_2           MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.2/self_attn/MatMul             MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.2/self_attn/MatMul_1           MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.2/self_attn/Add_2              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.2/self_attn/Add                Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.2/self_attn/Add_1              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.2/self_attn/Transpose_1        Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.2/self_attn/Transpose          Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.2/self_attn/Transpose_2        Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x32x100
/sem_seg_head/predictor/transformer_self_attention_layers.2/self_attn/Div                Div                    102,400          0.00%       102,400         0.00%       0           0.00%       8x100x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.2/self_attn/MatMul_3           MatMul                 2,560,000        0.00%       320,000         0.00%       0           0.00%       8x100x32       8x100x100
/sem_seg_head/predictor/transformer_self_attention_layers.2/self_attn/Softmax            Softmax                2,880,000        0.00%       320,000         0.00%       0           0.00%       8x100x100      8x100x100
/sem_seg_head/predictor/transformer_self_attention_layers.2/self_attn/MatMul_4           MatMul                 2,560,000        0.00%       102,400         0.00%       0           0.00%       8x100x100      8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.2/self_attn/Transpose_3        Transpose              0                0.00%       102,400         0.00%       0           0.00%       8x100x32       100x8x32
/sem_seg_head/predictor/transformer_self_attention_layers.2/self_attn/Gemm               Gemm                   6,579,200        0.00%       365,568         0.00%       65,792      0.08%       100x256        100x256
/sem_seg_head/predictor/transformer_self_attention_layers.2/Add_1                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.2/norm/ReduceMean              ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.2/norm/Sub                     Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.2/norm/Pow                     Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.2/norm/ReduceMean_1            ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.2/norm/Add                     Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.2/norm/Sqrt                    Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.2/norm/Div                     Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.2/norm/Mul                     Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.2/norm/Add_1                   Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.2/linear1/MatMul                          MatMul                 52,428,800       0.02%       2,916,352       0.01%       524,288     0.65%       100x1x256      100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.2/linear1/Add                             Add                    204,800          0.00%       827,392         0.00%       2,048       0.00%       2048           100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.2/Relu                                    Relu                   204,800          0.00%       819,200         0.00%       0           0.00%       100x1x2048     100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.2/linear2/MatMul                          MatMul                 52,428,800       0.02%       2,199,552       0.01%       524,288     0.65%       100x1x2048     100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.2/linear2/Add                             Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.2/Add                                     Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.2/norm/ReduceMean                         ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.2/norm/Sub                                Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.2/norm/Pow                                Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.2/norm/ReduceMean_1                       ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.2/norm/Add                                Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.2/norm/Sqrt                               Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.2/norm/Div                                Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.2/norm/Mul                                Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.2/norm/Add_1                              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_3/ReduceMean                                        ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.3/Add                         Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_3/Sub                                               Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.3/multihead_attn/MatMul       MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_3/Pow                                               Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.3/multihead_attn/Add          Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/decoder_norm_3/ReduceMean_1                                      ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/decoder_norm_3/Add                                               Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.3/multihead_attn/Transpose    Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/decoder_norm_3/Sqrt                                              Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.3/multihead_attn/Div          Div                    102,400          0.00%       102,400         0.00%       0           0.00%       8x100x32       8x100x32
/sem_seg_head/predictor/decoder_norm_3/Div                                               Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.3/multihead_attn/MatMul_3     MatMul                 26,214,400       0.01%       3,276,800       0.01%       0           0.00%       8x100x32       8x100x1024
/sem_seg_head/predictor/decoder_norm_3/Mul                                               Mul                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_3/Add_1                                             Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/Transpose_3                                                      Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x1x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.0_3/MatMul                                     MatMul                 6,553,600        0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.0_3/Add                                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       256            1x100x256
/sem_seg_head/predictor/mask_embed_3/Relu                                                Relu                   25,600           0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.1_3/MatMul                                     MatMul                 6,553,600        0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.1_3/Add                                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       256            1x100x256
/sem_seg_head/predictor/mask_embed_3/Relu_1                                              Relu                   25,600           0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.2_3/MatMul                                     MatMul                 6,553,600        0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.2_3/Add                                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       256            1x100x256
/sem_seg_head/predictor/MatMul_3                                                         MatMul                 1,677,721,600    0.62%       26,214,400      0.12%       0           0.00%       1x100x256      1x100x65536
/sem_seg_head/predictor/Resize_3                                                         Resize                 409,600          0.00%       409,600         0.00%       0           0.00%       1x100x256x256  1x100x32x32
/sem_seg_head/predictor/Sigmoid_3                                                        Sigmoid                3,276,800        0.00%       409,600         0.00%       0           0.00%       1x100x32x32    1x100x32x32
/sem_seg_head/predictor/Tile_5                                                           Tile                   0                0.00%       3,276,800       0.01%       0           0.00%       1x1x100x1024   1x8x100x1024
/sem_seg_head/predictor/Less_3                                                           Less                   819,200          0.00%       819,200         0.00%       0           0.00%       8x100x1024     8x100x1024
/sem_seg_head/predictor/ReduceSum_3                                                      ReduceSum              819,200          0.00%       6,400           0.00%       0           0.00%       8x100x1024     8x100
/sem_seg_head/predictor/Equal_3                                                          Equal                  800              0.00%       800             0.00%       0           0.00%       8x100          8x100
/sem_seg_head/predictor/Sub_3                                                            Sub                    800              0.00%       3,200           0.00%       0           0.00%       0              8x100
/sem_seg_head/predictor/Mul_3                                                            Mul                    819,200          0.00%       3,276,800       0.01%       0           0.00%       8x100x1024     8x100x1024
/sem_seg_head/predictor/transformer_cross_attention_layers.3/multihead_attn/Where        Where                  0                0.00%       3,276,800       0.01%       0           0.00%       8x100x1024     8x100x1024
/sem_seg_head/predictor/transformer_cross_attention_layers.3/multihead_attn/Add_3        Add                    819,200          0.00%       3,276,800       0.01%       0           0.00%       8x100x1024     8x100x1024
/sem_seg_head/predictor/transformer_cross_attention_layers.3/multihead_attn/Softmax      Softmax                29,491,200       0.01%       3,276,800       0.01%       0           0.00%       8x100x1024     8x100x1024
/sem_seg_head/predictor/transformer_cross_attention_layers.3/multihead_attn/MatMul_4     MatMul                 26,214,400       0.01%       102,400         0.00%       0           0.00%       8x100x1024     8x100x32
/sem_seg_head/predictor/transformer_cross_attention_layers.3/multihead_attn/Transpose_3  Transpose              0                0.00%       102,400         0.00%       0           0.00%       8x100x32       100x8x32
/sem_seg_head/predictor/transformer_cross_attention_layers.3/multihead_attn/Gemm         Gemm                   6,579,200        0.00%       365,568         0.00%       65,792      0.08%       100x256        100x256
/sem_seg_head/predictor/transformer_cross_attention_layers.3/Add_1                       Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.3/norm/ReduceMean             ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.3/norm/Sub                    Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.3/norm/Pow                    Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.3/norm/ReduceMean_1           ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.3/norm/Add                    Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.3/norm/Sqrt                   Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.3/norm/Div                    Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.3/norm/Mul                    Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.3/norm/Add_1                  Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.3/Add                          Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.3/self_attn/MatMul_2           MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.3/self_attn/MatMul             MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.3/self_attn/MatMul_1           MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.3/self_attn/Add_2              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.3/self_attn/Add                Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.3/self_attn/Add_1              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.3/self_attn/Transpose_1        Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.3/self_attn/Transpose          Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.3/self_attn/Transpose_2        Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x32x100
/sem_seg_head/predictor/transformer_self_attention_layers.3/self_attn/Div                Div                    102,400          0.00%       102,400         0.00%       0           0.00%       8x100x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.3/self_attn/MatMul_3           MatMul                 2,560,000        0.00%       320,000         0.00%       0           0.00%       8x100x32       8x100x100
/sem_seg_head/predictor/transformer_self_attention_layers.3/self_attn/Softmax            Softmax                2,880,000        0.00%       320,000         0.00%       0           0.00%       8x100x100      8x100x100
/sem_seg_head/predictor/transformer_self_attention_layers.3/self_attn/MatMul_4           MatMul                 2,560,000        0.00%       102,400         0.00%       0           0.00%       8x100x100      8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.3/self_attn/Transpose_3        Transpose              0                0.00%       102,400         0.00%       0           0.00%       8x100x32       100x8x32
/sem_seg_head/predictor/transformer_self_attention_layers.3/self_attn/Gemm               Gemm                   6,579,200        0.00%       365,568         0.00%       65,792      0.08%       100x256        100x256
/sem_seg_head/predictor/transformer_self_attention_layers.3/Add_1                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.3/norm/ReduceMean              ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.3/norm/Sub                     Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.3/norm/Pow                     Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.3/norm/ReduceMean_1            ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.3/norm/Add                     Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.3/norm/Sqrt                    Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.3/norm/Div                     Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.3/norm/Mul                     Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.3/norm/Add_1                   Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.3/linear1/MatMul                          MatMul                 52,428,800       0.02%       2,916,352       0.01%       524,288     0.65%       100x1x256      100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.3/linear1/Add                             Add                    204,800          0.00%       827,392         0.00%       2,048       0.00%       2048           100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.3/Relu                                    Relu                   204,800          0.00%       819,200         0.00%       0           0.00%       100x1x2048     100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.3/linear2/MatMul                          MatMul                 52,428,800       0.02%       2,199,552       0.01%       524,288     0.65%       100x1x2048     100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.3/linear2/Add                             Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.3/Add                                     Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.3/norm/ReduceMean                         ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.3/norm/Sub                                Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.3/norm/Pow                                Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.3/norm/ReduceMean_1                       ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.3/norm/Add                                Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.3/norm/Sqrt                               Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.3/norm/Div                                Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.3/norm/Mul                                Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.3/norm/Add_1                              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_4/ReduceMean                                        ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.4/Add                         Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_4/Sub                                               Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.4/multihead_attn/MatMul       MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_4/Pow                                               Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.4/multihead_attn/Add          Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/decoder_norm_4/ReduceMean_1                                      ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/decoder_norm_4/Add                                               Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.4/multihead_attn/Transpose    Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/decoder_norm_4/Sqrt                                              Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.4/multihead_attn/Div          Div                    102,400          0.00%       102,400         0.00%       0           0.00%       8x100x32       8x100x32
/sem_seg_head/predictor/decoder_norm_4/Div                                               Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.4/multihead_attn/MatMul_3     MatMul                 104,857,600      0.04%       13,107,200      0.06%       0           0.00%       8x100x32       8x100x4096
/sem_seg_head/predictor/decoder_norm_4/Mul                                               Mul                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_4/Add_1                                             Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/Transpose_4                                                      Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x1x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.0_4/MatMul                                     MatMul                 6,553,600        0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.0_4/Add                                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       256            1x100x256
/sem_seg_head/predictor/mask_embed_4/Relu                                                Relu                   25,600           0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.1_4/MatMul                                     MatMul                 6,553,600        0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.1_4/Add                                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       256            1x100x256
/sem_seg_head/predictor/mask_embed_4/Relu_1                                              Relu                   25,600           0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.2_4/MatMul                                     MatMul                 6,553,600        0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.2_4/Add                                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       256            1x100x256
/sem_seg_head/predictor/MatMul_4                                                         MatMul                 1,677,721,600    0.62%       26,214,400      0.12%       0           0.00%       1x100x256      1x100x65536
/sem_seg_head/predictor/Resize_4                                                         Resize                 1,638,400        0.00%       1,638,400       0.01%       0           0.00%       1x100x256x256  1x100x64x64
/sem_seg_head/predictor/Sigmoid_4                                                        Sigmoid                13,107,200       0.00%       1,638,400       0.01%       0           0.00%       1x100x64x64    1x100x64x64
/sem_seg_head/predictor/Tile_6                                                           Tile                   0                0.00%       13,107,200      0.06%       0           0.00%       1x1x100x4096   1x8x100x4096
/sem_seg_head/predictor/Less_4                                                           Less                   3,276,800        0.00%       3,276,800       0.01%       0           0.00%       8x100x4096     8x100x4096
/sem_seg_head/predictor/ReduceSum_4                                                      ReduceSum              3,276,800        0.00%       6,400           0.00%       0           0.00%       8x100x4096     8x100
/sem_seg_head/predictor/Equal_4                                                          Equal                  800              0.00%       800             0.00%       0           0.00%       8x100          8x100
/sem_seg_head/predictor/Sub_4                                                            Sub                    800              0.00%       3,200           0.00%       0           0.00%       0              8x100
/sem_seg_head/predictor/Mul_4                                                            Mul                    3,276,800        0.00%       13,107,200      0.06%       0           0.00%       8x100x4096     8x100x4096
/sem_seg_head/predictor/transformer_cross_attention_layers.4/multihead_attn/Where        Where                  0                0.00%       13,107,200      0.06%       0           0.00%       8x100x4096     8x100x4096
/sem_seg_head/predictor/transformer_cross_attention_layers.4/multihead_attn/Add_3        Add                    3,276,800        0.00%       13,107,200      0.06%       0           0.00%       8x100x4096     8x100x4096
/sem_seg_head/predictor/transformer_cross_attention_layers.4/multihead_attn/Softmax      Softmax                117,964,800      0.04%       13,107,200      0.06%       0           0.00%       8x100x4096     8x100x4096
/sem_seg_head/predictor/transformer_cross_attention_layers.4/multihead_attn/MatMul_4     MatMul                 104,857,600      0.04%       102,400         0.00%       0           0.00%       8x100x4096     8x100x32
/sem_seg_head/predictor/transformer_cross_attention_layers.4/multihead_attn/Transpose_3  Transpose              0                0.00%       102,400         0.00%       0           0.00%       8x100x32       100x8x32
/sem_seg_head/predictor/transformer_cross_attention_layers.4/multihead_attn/Gemm         Gemm                   6,579,200        0.00%       365,568         0.00%       65,792      0.08%       100x256        100x256
/sem_seg_head/predictor/transformer_cross_attention_layers.4/Add_1                       Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.4/norm/ReduceMean             ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.4/norm/Sub                    Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.4/norm/Pow                    Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.4/norm/ReduceMean_1           ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.4/norm/Add                    Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.4/norm/Sqrt                   Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.4/norm/Div                    Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.4/norm/Mul                    Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.4/norm/Add_1                  Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.4/Add                          Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.4/self_attn/MatMul_2           MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.4/self_attn/MatMul             MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.4/self_attn/MatMul_1           MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.4/self_attn/Add_2              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.4/self_attn/Add                Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.4/self_attn/Add_1              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.4/self_attn/Transpose_1        Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.4/self_attn/Transpose          Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.4/self_attn/Transpose_2        Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x32x100
/sem_seg_head/predictor/transformer_self_attention_layers.4/self_attn/Div                Div                    102,400          0.00%       102,400         0.00%       0           0.00%       8x100x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.4/self_attn/MatMul_3           MatMul                 2,560,000        0.00%       320,000         0.00%       0           0.00%       8x100x32       8x100x100
/sem_seg_head/predictor/transformer_self_attention_layers.4/self_attn/Softmax            Softmax                2,880,000        0.00%       320,000         0.00%       0           0.00%       8x100x100      8x100x100
/sem_seg_head/predictor/transformer_self_attention_layers.4/self_attn/MatMul_4           MatMul                 2,560,000        0.00%       102,400         0.00%       0           0.00%       8x100x100      8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.4/self_attn/Transpose_3        Transpose              0                0.00%       102,400         0.00%       0           0.00%       8x100x32       100x8x32
/sem_seg_head/predictor/transformer_self_attention_layers.4/self_attn/Gemm               Gemm                   6,579,200        0.00%       365,568         0.00%       65,792      0.08%       100x256        100x256
/sem_seg_head/predictor/transformer_self_attention_layers.4/Add_1                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.4/norm/ReduceMean              ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.4/norm/Sub                     Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.4/norm/Pow                     Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.4/norm/ReduceMean_1            ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.4/norm/Add                     Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.4/norm/Sqrt                    Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.4/norm/Div                     Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.4/norm/Mul                     Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.4/norm/Add_1                   Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.4/linear1/MatMul                          MatMul                 52,428,800       0.02%       2,916,352       0.01%       524,288     0.65%       100x1x256      100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.4/linear1/Add                             Add                    204,800          0.00%       827,392         0.00%       2,048       0.00%       2048           100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.4/Relu                                    Relu                   204,800          0.00%       819,200         0.00%       0           0.00%       100x1x2048     100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.4/linear2/MatMul                          MatMul                 52,428,800       0.02%       2,199,552       0.01%       524,288     0.65%       100x1x2048     100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.4/linear2/Add                             Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.4/Add                                     Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.4/norm/ReduceMean                         ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.4/norm/Sub                                Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.4/norm/Pow                                Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.4/norm/ReduceMean_1                       ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.4/norm/Add                                Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.4/norm/Sqrt                               Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.4/norm/Div                                Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.4/norm/Mul                                Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.4/norm/Add_1                              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_5/ReduceMean                                        ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.5/Add                         Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_5/Sub                                               Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.5/multihead_attn/MatMul       MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_5/Pow                                               Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.5/multihead_attn/Add          Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/decoder_norm_5/ReduceMean_1                                      ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/decoder_norm_5/Add                                               Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.5/multihead_attn/Transpose    Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/decoder_norm_5/Sqrt                                              Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.5/multihead_attn/Div          Div                    102,400          0.00%       102,400         0.00%       0           0.00%       8x100x32       8x100x32
/sem_seg_head/predictor/decoder_norm_5/Div                                               Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.5/multihead_attn/MatMul_3     MatMul                 419,430,400      0.16%       52,428,800      0.24%       0           0.00%       8x100x32       8x100x16384
/sem_seg_head/predictor/decoder_norm_5/Mul                                               Mul                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_5/Add_1                                             Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/Transpose_5                                                      Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x1x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.0_5/MatMul                                     MatMul                 6,553,600        0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.0_5/Add                                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       256            1x100x256
/sem_seg_head/predictor/mask_embed_5/Relu                                                Relu                   25,600           0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.1_5/MatMul                                     MatMul                 6,553,600        0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.1_5/Add                                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       256            1x100x256
/sem_seg_head/predictor/mask_embed_5/Relu_1                                              Relu                   25,600           0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.2_5/MatMul                                     MatMul                 6,553,600        0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.2_5/Add                                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       256            1x100x256
/sem_seg_head/predictor/MatMul_5                                                         MatMul                 1,677,721,600    0.62%       26,214,400      0.12%       0           0.00%       1x100x256      1x100x65536
/sem_seg_head/predictor/Resize_5                                                         Resize                 6,553,600        0.00%       6,553,600       0.03%       0           0.00%       1x100x256x256  1x100x128x128
/sem_seg_head/predictor/Sigmoid_5                                                        Sigmoid                52,428,800       0.02%       6,553,600       0.03%       0           0.00%       1x100x128x128  1x100x128x128
/sem_seg_head/predictor/Tile_7                                                           Tile                   0                0.00%       52,428,800      0.24%       0           0.00%       1x1x100x16384  1x8x100x16384
/sem_seg_head/predictor/Less_5                                                           Less                   13,107,200       0.00%       13,107,200      0.06%       0           0.00%       8x100x16384    8x100x16384
/sem_seg_head/predictor/ReduceSum_5                                                      ReduceSum              13,107,200       0.00%       6,400           0.00%       0           0.00%       8x100x16384    8x100
/sem_seg_head/predictor/Equal_5                                                          Equal                  800              0.00%       800             0.00%       0           0.00%       8x100          8x100
/sem_seg_head/predictor/Sub_5                                                            Sub                    800              0.00%       3,200           0.00%       0           0.00%       0              8x100
/sem_seg_head/predictor/Mul_5                                                            Mul                    13,107,200       0.00%       52,428,800      0.24%       0           0.00%       8x100x16384    8x100x16384
/sem_seg_head/predictor/transformer_cross_attention_layers.5/multihead_attn/Where        Where                  0                0.00%       52,428,800      0.24%       0           0.00%       8x100x16384    8x100x16384
/sem_seg_head/predictor/transformer_cross_attention_layers.5/multihead_attn/Add_3        Add                    13,107,200       0.00%       52,428,800      0.24%       0           0.00%       8x100x16384    8x100x16384
/sem_seg_head/predictor/transformer_cross_attention_layers.5/multihead_attn/Softmax      Softmax                471,859,200      0.18%       52,428,800      0.24%       0           0.00%       8x100x16384    8x100x16384
/sem_seg_head/predictor/transformer_cross_attention_layers.5/multihead_attn/MatMul_4     MatMul                 419,430,400      0.16%       102,400         0.00%       0           0.00%       8x100x16384    8x100x32
/sem_seg_head/predictor/transformer_cross_attention_layers.5/multihead_attn/Transpose_3  Transpose              0                0.00%       102,400         0.00%       0           0.00%       8x100x32       100x8x32
/sem_seg_head/predictor/transformer_cross_attention_layers.5/multihead_attn/Gemm         Gemm                   6,579,200        0.00%       365,568         0.00%       65,792      0.08%       100x256        100x256
/sem_seg_head/predictor/transformer_cross_attention_layers.5/Add_1                       Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.5/norm/ReduceMean             ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.5/norm/Sub                    Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.5/norm/Pow                    Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.5/norm/ReduceMean_1           ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.5/norm/Add                    Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.5/norm/Sqrt                   Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.5/norm/Div                    Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.5/norm/Mul                    Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.5/norm/Add_1                  Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.5/Add                          Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.5/self_attn/MatMul_2           MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.5/self_attn/MatMul             MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.5/self_attn/MatMul_1           MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.5/self_attn/Add_2              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.5/self_attn/Add                Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.5/self_attn/Add_1              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.5/self_attn/Transpose_1        Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.5/self_attn/Transpose          Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.5/self_attn/Transpose_2        Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x32x100
/sem_seg_head/predictor/transformer_self_attention_layers.5/self_attn/Div                Div                    102,400          0.00%       102,400         0.00%       0           0.00%       8x100x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.5/self_attn/MatMul_3           MatMul                 2,560,000        0.00%       320,000         0.00%       0           0.00%       8x100x32       8x100x100
/sem_seg_head/predictor/transformer_self_attention_layers.5/self_attn/Softmax            Softmax                2,880,000        0.00%       320,000         0.00%       0           0.00%       8x100x100      8x100x100
/sem_seg_head/predictor/transformer_self_attention_layers.5/self_attn/MatMul_4           MatMul                 2,560,000        0.00%       102,400         0.00%       0           0.00%       8x100x100      8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.5/self_attn/Transpose_3        Transpose              0                0.00%       102,400         0.00%       0           0.00%       8x100x32       100x8x32
/sem_seg_head/predictor/transformer_self_attention_layers.5/self_attn/Gemm               Gemm                   6,579,200        0.00%       365,568         0.00%       65,792      0.08%       100x256        100x256
/sem_seg_head/predictor/transformer_self_attention_layers.5/Add_1                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.5/norm/ReduceMean              ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.5/norm/Sub                     Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.5/norm/Pow                     Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.5/norm/ReduceMean_1            ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.5/norm/Add                     Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.5/norm/Sqrt                    Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.5/norm/Div                     Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.5/norm/Mul                     Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.5/norm/Add_1                   Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.5/linear1/MatMul                          MatMul                 52,428,800       0.02%       2,916,352       0.01%       524,288     0.65%       100x1x256      100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.5/linear1/Add                             Add                    204,800          0.00%       827,392         0.00%       2,048       0.00%       2048           100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.5/Relu                                    Relu                   204,800          0.00%       819,200         0.00%       0           0.00%       100x1x2048     100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.5/linear2/MatMul                          MatMul                 52,428,800       0.02%       2,199,552       0.01%       524,288     0.65%       100x1x2048     100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.5/linear2/Add                             Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.5/Add                                     Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.5/norm/ReduceMean                         ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.5/norm/Sub                                Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.5/norm/Pow                                Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.5/norm/ReduceMean_1                       ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.5/norm/Add                                Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.5/norm/Sqrt                               Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.5/norm/Div                                Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.5/norm/Mul                                Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.5/norm/Add_1                              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_6/ReduceMean                                        ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.6/Add                         Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_6/Sub                                               Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.6/multihead_attn/MatMul       MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_6/Pow                                               Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.6/multihead_attn/Add          Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/decoder_norm_6/ReduceMean_1                                      ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/decoder_norm_6/Add                                               Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.6/multihead_attn/Transpose    Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/decoder_norm_6/Sqrt                                              Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.6/multihead_attn/Div          Div                    102,400          0.00%       102,400         0.00%       0           0.00%       8x100x32       8x100x32
/sem_seg_head/predictor/decoder_norm_6/Div                                               Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.6/multihead_attn/MatMul_3     MatMul                 26,214,400       0.01%       3,276,800       0.01%       0           0.00%       8x100x32       8x100x1024
/sem_seg_head/predictor/decoder_norm_6/Mul                                               Mul                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_6/Add_1                                             Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/Transpose_6                                                      Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x1x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.0_6/MatMul                                     MatMul                 6,553,600        0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.0_6/Add                                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       256            1x100x256
/sem_seg_head/predictor/mask_embed_6/Relu                                                Relu                   25,600           0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.1_6/MatMul                                     MatMul                 6,553,600        0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.1_6/Add                                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       256            1x100x256
/sem_seg_head/predictor/mask_embed_6/Relu_1                                              Relu                   25,600           0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.2_6/MatMul                                     MatMul                 6,553,600        0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.2_6/Add                                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       256            1x100x256
/sem_seg_head/predictor/MatMul_6                                                         MatMul                 1,677,721,600    0.62%       26,214,400      0.12%       0           0.00%       1x100x256      1x100x65536
/sem_seg_head/predictor/Resize_6                                                         Resize                 409,600          0.00%       409,600         0.00%       0           0.00%       1x100x256x256  1x100x32x32
/sem_seg_head/predictor/Sigmoid_6                                                        Sigmoid                3,276,800        0.00%       409,600         0.00%       0           0.00%       1x100x32x32    1x100x32x32
/sem_seg_head/predictor/Tile_8                                                           Tile                   0                0.00%       3,276,800       0.01%       0           0.00%       1x1x100x1024   1x8x100x1024
/sem_seg_head/predictor/Less_6                                                           Less                   819,200          0.00%       819,200         0.00%       0           0.00%       8x100x1024     8x100x1024
/sem_seg_head/predictor/ReduceSum_6                                                      ReduceSum              819,200          0.00%       6,400           0.00%       0           0.00%       8x100x1024     8x100
/sem_seg_head/predictor/Equal_6                                                          Equal                  800              0.00%       800             0.00%       0           0.00%       8x100          8x100
/sem_seg_head/predictor/Sub_6                                                            Sub                    800              0.00%       3,200           0.00%       0           0.00%       0              8x100
/sem_seg_head/predictor/Mul_6                                                            Mul                    819,200          0.00%       3,276,800       0.01%       0           0.00%       8x100x1024     8x100x1024
/sem_seg_head/predictor/transformer_cross_attention_layers.6/multihead_attn/Where        Where                  0                0.00%       3,276,800       0.01%       0           0.00%       8x100x1024     8x100x1024
/sem_seg_head/predictor/transformer_cross_attention_layers.6/multihead_attn/Add_3        Add                    819,200          0.00%       3,276,800       0.01%       0           0.00%       8x100x1024     8x100x1024
/sem_seg_head/predictor/transformer_cross_attention_layers.6/multihead_attn/Softmax      Softmax                29,491,200       0.01%       3,276,800       0.01%       0           0.00%       8x100x1024     8x100x1024
/sem_seg_head/predictor/transformer_cross_attention_layers.6/multihead_attn/MatMul_4     MatMul                 26,214,400       0.01%       102,400         0.00%       0           0.00%       8x100x1024     8x100x32
/sem_seg_head/predictor/transformer_cross_attention_layers.6/multihead_attn/Transpose_3  Transpose              0                0.00%       102,400         0.00%       0           0.00%       8x100x32       100x8x32
/sem_seg_head/predictor/transformer_cross_attention_layers.6/multihead_attn/Gemm         Gemm                   6,579,200        0.00%       365,568         0.00%       65,792      0.08%       100x256        100x256
/sem_seg_head/predictor/transformer_cross_attention_layers.6/Add_1                       Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.6/norm/ReduceMean             ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.6/norm/Sub                    Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.6/norm/Pow                    Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.6/norm/ReduceMean_1           ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.6/norm/Add                    Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.6/norm/Sqrt                   Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.6/norm/Div                    Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.6/norm/Mul                    Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.6/norm/Add_1                  Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.6/Add                          Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.6/self_attn/MatMul_2           MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.6/self_attn/MatMul             MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.6/self_attn/MatMul_1           MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.6/self_attn/Add_2              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.6/self_attn/Add                Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.6/self_attn/Add_1              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.6/self_attn/Transpose_1        Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.6/self_attn/Transpose          Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.6/self_attn/Transpose_2        Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x32x100
/sem_seg_head/predictor/transformer_self_attention_layers.6/self_attn/Div                Div                    102,400          0.00%       102,400         0.00%       0           0.00%       8x100x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.6/self_attn/MatMul_3           MatMul                 2,560,000        0.00%       320,000         0.00%       0           0.00%       8x100x32       8x100x100
/sem_seg_head/predictor/transformer_self_attention_layers.6/self_attn/Softmax            Softmax                2,880,000        0.00%       320,000         0.00%       0           0.00%       8x100x100      8x100x100
/sem_seg_head/predictor/transformer_self_attention_layers.6/self_attn/MatMul_4           MatMul                 2,560,000        0.00%       102,400         0.00%       0           0.00%       8x100x100      8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.6/self_attn/Transpose_3        Transpose              0                0.00%       102,400         0.00%       0           0.00%       8x100x32       100x8x32
/sem_seg_head/predictor/transformer_self_attention_layers.6/self_attn/Gemm               Gemm                   6,579,200        0.00%       365,568         0.00%       65,792      0.08%       100x256        100x256
/sem_seg_head/predictor/transformer_self_attention_layers.6/Add_1                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.6/norm/ReduceMean              ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.6/norm/Sub                     Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.6/norm/Pow                     Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.6/norm/ReduceMean_1            ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.6/norm/Add                     Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.6/norm/Sqrt                    Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.6/norm/Div                     Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.6/norm/Mul                     Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.6/norm/Add_1                   Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.6/linear1/MatMul                          MatMul                 52,428,800       0.02%       2,916,352       0.01%       524,288     0.65%       100x1x256      100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.6/linear1/Add                             Add                    204,800          0.00%       827,392         0.00%       2,048       0.00%       2048           100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.6/Relu                                    Relu                   204,800          0.00%       819,200         0.00%       0           0.00%       100x1x2048     100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.6/linear2/MatMul                          MatMul                 52,428,800       0.02%       2,199,552       0.01%       524,288     0.65%       100x1x2048     100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.6/linear2/Add                             Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.6/Add                                     Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.6/norm/ReduceMean                         ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.6/norm/Sub                                Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.6/norm/Pow                                Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.6/norm/ReduceMean_1                       ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.6/norm/Add                                Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.6/norm/Sqrt                               Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.6/norm/Div                                Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.6/norm/Mul                                Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.6/norm/Add_1                              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_7/ReduceMean                                        ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.7/Add                         Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_7/Sub                                               Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.7/multihead_attn/MatMul       MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_7/Pow                                               Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.7/multihead_attn/Add          Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/decoder_norm_7/ReduceMean_1                                      ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/decoder_norm_7/Add                                               Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.7/multihead_attn/Transpose    Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/decoder_norm_7/Sqrt                                              Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.7/multihead_attn/Div          Div                    102,400          0.00%       102,400         0.00%       0           0.00%       8x100x32       8x100x32
/sem_seg_head/predictor/decoder_norm_7/Div                                               Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.7/multihead_attn/MatMul_3     MatMul                 104,857,600      0.04%       13,107,200      0.06%       0           0.00%       8x100x32       8x100x4096
/sem_seg_head/predictor/decoder_norm_7/Mul                                               Mul                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_7/Add_1                                             Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/Transpose_7                                                      Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x1x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.0_7/MatMul                                     MatMul                 6,553,600        0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.0_7/Add                                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       256            1x100x256
/sem_seg_head/predictor/mask_embed_7/Relu                                                Relu                   25,600           0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.1_7/MatMul                                     MatMul                 6,553,600        0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.1_7/Add                                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       256            1x100x256
/sem_seg_head/predictor/mask_embed_7/Relu_1                                              Relu                   25,600           0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.2_7/MatMul                                     MatMul                 6,553,600        0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.2_7/Add                                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       256            1x100x256
/sem_seg_head/predictor/MatMul_7                                                         MatMul                 1,677,721,600    0.62%       26,214,400      0.12%       0           0.00%       1x100x256      1x100x65536
/sem_seg_head/predictor/Resize_7                                                         Resize                 1,638,400        0.00%       1,638,400       0.01%       0           0.00%       1x100x256x256  1x100x64x64
/sem_seg_head/predictor/Sigmoid_7                                                        Sigmoid                13,107,200       0.00%       1,638,400       0.01%       0           0.00%       1x100x64x64    1x100x64x64
/sem_seg_head/predictor/Tile_9                                                           Tile                   0                0.00%       13,107,200      0.06%       0           0.00%       1x1x100x4096   1x8x100x4096
/sem_seg_head/predictor/Less_7                                                           Less                   3,276,800        0.00%       3,276,800       0.01%       0           0.00%       8x100x4096     8x100x4096
/sem_seg_head/predictor/ReduceSum_7                                                      ReduceSum              3,276,800        0.00%       6,400           0.00%       0           0.00%       8x100x4096     8x100
/sem_seg_head/predictor/Equal_7                                                          Equal                  800              0.00%       800             0.00%       0           0.00%       8x100          8x100
/sem_seg_head/predictor/Sub_7                                                            Sub                    800              0.00%       3,200           0.00%       0           0.00%       0              8x100
/sem_seg_head/predictor/Mul_7                                                            Mul                    3,276,800        0.00%       13,107,200      0.06%       0           0.00%       8x100x4096     8x100x4096
/sem_seg_head/predictor/transformer_cross_attention_layers.7/multihead_attn/Where        Where                  0                0.00%       13,107,200      0.06%       0           0.00%       8x100x4096     8x100x4096
/sem_seg_head/predictor/transformer_cross_attention_layers.7/multihead_attn/Add_3        Add                    3,276,800        0.00%       13,107,200      0.06%       0           0.00%       8x100x4096     8x100x4096
/sem_seg_head/predictor/transformer_cross_attention_layers.7/multihead_attn/Softmax      Softmax                117,964,800      0.04%       13,107,200      0.06%       0           0.00%       8x100x4096     8x100x4096
/sem_seg_head/predictor/transformer_cross_attention_layers.7/multihead_attn/MatMul_4     MatMul                 104,857,600      0.04%       102,400         0.00%       0           0.00%       8x100x4096     8x100x32
/sem_seg_head/predictor/transformer_cross_attention_layers.7/multihead_attn/Transpose_3  Transpose              0                0.00%       102,400         0.00%       0           0.00%       8x100x32       100x8x32
/sem_seg_head/predictor/transformer_cross_attention_layers.7/multihead_attn/Gemm         Gemm                   6,579,200        0.00%       365,568         0.00%       65,792      0.08%       100x256        100x256
/sem_seg_head/predictor/transformer_cross_attention_layers.7/Add_1                       Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.7/norm/ReduceMean             ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.7/norm/Sub                    Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.7/norm/Pow                    Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.7/norm/ReduceMean_1           ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.7/norm/Add                    Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.7/norm/Sqrt                   Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.7/norm/Div                    Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.7/norm/Mul                    Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.7/norm/Add_1                  Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.7/Add                          Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.7/self_attn/MatMul_2           MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.7/self_attn/MatMul             MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.7/self_attn/MatMul_1           MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.7/self_attn/Add_2              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.7/self_attn/Add                Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.7/self_attn/Add_1              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.7/self_attn/Transpose_1        Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.7/self_attn/Transpose          Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.7/self_attn/Transpose_2        Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x32x100
/sem_seg_head/predictor/transformer_self_attention_layers.7/self_attn/Div                Div                    102,400          0.00%       102,400         0.00%       0           0.00%       8x100x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.7/self_attn/MatMul_3           MatMul                 2,560,000        0.00%       320,000         0.00%       0           0.00%       8x100x32       8x100x100
/sem_seg_head/predictor/transformer_self_attention_layers.7/self_attn/Softmax            Softmax                2,880,000        0.00%       320,000         0.00%       0           0.00%       8x100x100      8x100x100
/sem_seg_head/predictor/transformer_self_attention_layers.7/self_attn/MatMul_4           MatMul                 2,560,000        0.00%       102,400         0.00%       0           0.00%       8x100x100      8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.7/self_attn/Transpose_3        Transpose              0                0.00%       102,400         0.00%       0           0.00%       8x100x32       100x8x32
/sem_seg_head/predictor/transformer_self_attention_layers.7/self_attn/Gemm               Gemm                   6,579,200        0.00%       365,568         0.00%       65,792      0.08%       100x256        100x256
/sem_seg_head/predictor/transformer_self_attention_layers.7/Add_1                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.7/norm/ReduceMean              ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.7/norm/Sub                     Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.7/norm/Pow                     Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.7/norm/ReduceMean_1            ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.7/norm/Add                     Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.7/norm/Sqrt                    Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.7/norm/Div                     Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.7/norm/Mul                     Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.7/norm/Add_1                   Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.7/linear1/MatMul                          MatMul                 52,428,800       0.02%       2,916,352       0.01%       524,288     0.65%       100x1x256      100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.7/linear1/Add                             Add                    204,800          0.00%       827,392         0.00%       2,048       0.00%       2048           100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.7/Relu                                    Relu                   204,800          0.00%       819,200         0.00%       0           0.00%       100x1x2048     100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.7/linear2/MatMul                          MatMul                 52,428,800       0.02%       2,199,552       0.01%       524,288     0.65%       100x1x2048     100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.7/linear2/Add                             Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.7/Add                                     Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.7/norm/ReduceMean                         ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.7/norm/Sub                                Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.7/norm/Pow                                Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.7/norm/ReduceMean_1                       ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.7/norm/Add                                Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.7/norm/Sqrt                               Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.7/norm/Div                                Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.7/norm/Mul                                Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.7/norm/Add_1                              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_8/ReduceMean                                        ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.8/Add                         Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_8/Sub                                               Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.8/multihead_attn/MatMul       MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_8/Pow                                               Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.8/multihead_attn/Add          Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/decoder_norm_8/ReduceMean_1                                      ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/decoder_norm_8/Add                                               Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.8/multihead_attn/Transpose    Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/decoder_norm_8/Sqrt                                              Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.8/multihead_attn/Div          Div                    102,400          0.00%       102,400         0.00%       0           0.00%       8x100x32       8x100x32
/sem_seg_head/predictor/decoder_norm_8/Div                                               Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.8/multihead_attn/MatMul_3     MatMul                 419,430,400      0.16%       52,428,800      0.24%       0           0.00%       8x100x32       8x100x16384
/sem_seg_head/predictor/decoder_norm_8/Mul                                               Mul                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_8/Add_1                                             Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/Transpose_8                                                      Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x1x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.0_8/MatMul                                     MatMul                 6,553,600        0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.0_8/Add                                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       256            1x100x256
/sem_seg_head/predictor/mask_embed_8/Relu                                                Relu                   25,600           0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.1_8/MatMul                                     MatMul                 6,553,600        0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.1_8/Add                                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       256            1x100x256
/sem_seg_head/predictor/mask_embed_8/Relu_1                                              Relu                   25,600           0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.2_8/MatMul                                     MatMul                 6,553,600        0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.2_8/Add                                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       256            1x100x256
/sem_seg_head/predictor/MatMul_8                                                         MatMul                 1,677,721,600    0.62%       26,214,400      0.12%       0           0.00%       1x100x256      1x100x65536
/sem_seg_head/predictor/Resize_8                                                         Resize                 6,553,600        0.00%       6,553,600       0.03%       0           0.00%       1x100x256x256  1x100x128x128
/sem_seg_head/predictor/Sigmoid_8                                                        Sigmoid                52,428,800       0.02%       6,553,600       0.03%       0           0.00%       1x100x128x128  1x100x128x128
/sem_seg_head/predictor/Tile_10                                                          Tile                   0                0.00%       52,428,800      0.24%       0           0.00%       1x1x100x16384  1x8x100x16384
/sem_seg_head/predictor/Less_8                                                           Less                   13,107,200       0.00%       13,107,200      0.06%       0           0.00%       8x100x16384    8x100x16384
/sem_seg_head/predictor/ReduceSum_8                                                      ReduceSum              13,107,200       0.00%       6,400           0.00%       0           0.00%       8x100x16384    8x100
/sem_seg_head/predictor/Equal_8                                                          Equal                  800              0.00%       800             0.00%       0           0.00%       8x100          8x100
/sem_seg_head/predictor/Sub_8                                                            Sub                    800              0.00%       3,200           0.00%       0           0.00%       0              8x100
/sem_seg_head/predictor/Mul_8                                                            Mul                    13,107,200       0.00%       52,428,800      0.24%       0           0.00%       8x100x16384    8x100x16384
/sem_seg_head/predictor/transformer_cross_attention_layers.8/multihead_attn/Where        Where                  0                0.00%       52,428,800      0.24%       0           0.00%       8x100x16384    8x100x16384
/sem_seg_head/predictor/transformer_cross_attention_layers.8/multihead_attn/Add_3        Add                    13,107,200       0.00%       52,428,800      0.24%       0           0.00%       8x100x16384    8x100x16384
/sem_seg_head/predictor/transformer_cross_attention_layers.8/multihead_attn/Softmax      Softmax                471,859,200      0.18%       52,428,800      0.24%       0           0.00%       8x100x16384    8x100x16384
/sem_seg_head/predictor/transformer_cross_attention_layers.8/multihead_attn/MatMul_4     MatMul                 419,430,400      0.16%       102,400         0.00%       0           0.00%       8x100x16384    8x100x32
/sem_seg_head/predictor/transformer_cross_attention_layers.8/multihead_attn/Transpose_3  Transpose              0                0.00%       102,400         0.00%       0           0.00%       8x100x32       100x8x32
/sem_seg_head/predictor/transformer_cross_attention_layers.8/multihead_attn/Gemm         Gemm                   6,579,200        0.00%       365,568         0.00%       65,792      0.08%       100x256        100x256
/sem_seg_head/predictor/transformer_cross_attention_layers.8/Add_1                       Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.8/norm/ReduceMean             ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.8/norm/Sub                    Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.8/norm/Pow                    Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.8/norm/ReduceMean_1           ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.8/norm/Add                    Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.8/norm/Sqrt                   Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_cross_attention_layers.8/norm/Div                    Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.8/norm/Mul                    Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_cross_attention_layers.8/norm/Add_1                  Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.8/Add                          Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.8/self_attn/MatMul_2           MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.8/self_attn/MatMul             MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.8/self_attn/MatMul_1           MatMul                 6,553,600        0.00%       364,544         0.00%       65,536      0.08%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.8/self_attn/Add_2              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.8/self_attn/Add                Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.8/self_attn/Add_1              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.8/self_attn/Transpose_1        Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.8/self_attn/Transpose          Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.8/self_attn/Transpose_2        Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x8x32       8x32x100
/sem_seg_head/predictor/transformer_self_attention_layers.8/self_attn/Div                Div                    102,400          0.00%       102,400         0.00%       0           0.00%       8x100x32       8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.8/self_attn/MatMul_3           MatMul                 2,560,000        0.00%       320,000         0.00%       0           0.00%       8x100x32       8x100x100
/sem_seg_head/predictor/transformer_self_attention_layers.8/self_attn/Softmax            Softmax                2,880,000        0.00%       320,000         0.00%       0           0.00%       8x100x100      8x100x100
/sem_seg_head/predictor/transformer_self_attention_layers.8/self_attn/MatMul_4           MatMul                 2,560,000        0.00%       102,400         0.00%       0           0.00%       8x100x100      8x100x32
/sem_seg_head/predictor/transformer_self_attention_layers.8/self_attn/Transpose_3        Transpose              0                0.00%       102,400         0.00%       0           0.00%       8x100x32       100x8x32
/sem_seg_head/predictor/transformer_self_attention_layers.8/self_attn/Gemm               Gemm                   6,579,200        0.00%       365,568         0.00%       65,792      0.08%       100x256        100x256
/sem_seg_head/predictor/transformer_self_attention_layers.8/Add_1                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.8/norm/ReduceMean              ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.8/norm/Sub                     Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.8/norm/Pow                     Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.8/norm/ReduceMean_1            ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.8/norm/Add                     Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.8/norm/Sqrt                    Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_self_attention_layers.8/norm/Div                     Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.8/norm/Mul                     Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_self_attention_layers.8/norm/Add_1                   Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.8/linear1/MatMul                          MatMul                 52,428,800       0.02%       2,916,352       0.01%       524,288     0.65%       100x1x256      100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.8/linear1/Add                             Add                    204,800          0.00%       827,392         0.00%       2,048       0.00%       2048           100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.8/Relu                                    Relu                   204,800          0.00%       819,200         0.00%       0           0.00%       100x1x2048     100x1x2048
/sem_seg_head/predictor/transformer_ffn_layers.8/linear2/MatMul                          MatMul                 52,428,800       0.02%       2,199,552       0.01%       524,288     0.65%       100x1x2048     100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.8/linear2/Add                             Add                    25,600           0.00%       103,424         0.00%       256         0.00%       256            100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.8/Add                                     Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.8/norm/ReduceMean                         ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.8/norm/Sub                                Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.8/norm/Pow                                Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.8/norm/ReduceMean_1                       ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.8/norm/Add                                Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.8/norm/Sqrt                               Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/transformer_ffn_layers.8/norm/Div                                Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.8/norm/Mul                                Mul                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/transformer_ffn_layers.8/norm/Add_1                              Add                    25,600           0.00%       103,424         0.00%       256         0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_9/ReduceMean                                        ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/decoder_norm_9/Sub                                               Sub                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_9/Pow                                               Pow                    819,200          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_9/ReduceMean_1                                      ReduceMean             25,600           0.00%       400             0.00%       0           0.00%       100x1x256      100x1x1
/sem_seg_head/predictor/decoder_norm_9/Add                                               Add                    100              0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/decoder_norm_9/Sqrt                                              Sqrt                   2,400            0.00%       400             0.00%       0           0.00%       100x1x1        100x1x1
/sem_seg_head/predictor/decoder_norm_9/Div                                               Div                    102,400          0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_9/Mul                                               Mul                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/decoder_norm_9/Add_1                                             Add                    25,600           0.00%       102,400         0.00%       0           0.00%       100x1x256      100x1x256
/sem_seg_head/predictor/Transpose_9                                                      Transpose              0                0.00%       102,400         0.00%       0           0.00%       100x1x256      1x100x256
/sem_seg_head/predictor/class_embed/MatMul                                               MatMul                 2,073,600        0.00%       115,344         0.00%       20,736      0.03%       1x100x256      1x100x81
/sem_seg_head/predictor/mask_embed/layers.0_9/MatMul                                     MatMul                 6,553,600        0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/class_embed/Add                                                  Add                    8,100            0.00%       32,724          0.00%       81          0.00%       81             1x100x81
/sem_seg_head/predictor/mask_embed/layers.0_9/Add                                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       256            1x100x256
/sem_seg_head/predictor/mask_embed_9/Relu                                                Relu                   25,600           0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.1_9/MatMul                                     MatMul                 6,553,600        0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.1_9/Add                                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       256            1x100x256
/sem_seg_head/predictor/mask_embed_9/Relu_1                                              Relu                   25,600           0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.2_9/MatMul                                     MatMul                 6,553,600        0.00%       102,400         0.00%       0           0.00%       1x100x256      1x100x256
/sem_seg_head/predictor/mask_embed/layers.2_9/Add                                        Add                    25,600           0.00%       102,400         0.00%       0           0.00%       256            1x100x256
/sem_seg_head/predictor/MatMul_9                                                         MatMul                 1,677,721,600    0.62%       26,214,400      0.12%       0           0.00%       1x100x256      1x100x65536
Total                                                                                    _                      268,638,456,180  100%        22,133,077,116  100%        80,473,952  100%        _              _
